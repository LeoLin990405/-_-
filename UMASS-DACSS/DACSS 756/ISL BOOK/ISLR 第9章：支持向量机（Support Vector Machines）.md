# ISLR 第9章：支持向量机（Support Vector Machines）

## 9.1 最大间隔分类器

### 9.1.1 超平面定义
在p维空间中的超平面方程：

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0$$

分类规则：
- 如果 $f(X) = \beta_0 + \sum_{j=1}^p \beta_j X_j > 0$，预测类别1
- 如果 $f(X) < 0$，预测类别-1

### 9.1.2 最大间隔分类器优化问题

$$\max_{\beta_0, \beta_1, ..., \beta_p, \|\beta\|=1} M$$

约束条件：

$$y_i(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}) \geq M, \quad \forall i=1,...,n$$

其中：
- $M$ 是间隔大小
- $y_i \in \{-1,1\}$ 是类别标签
- $\|\beta\|=1$ 是归一化约束

## 9.2 支持向量分类器（软间隔）

### 9.2.1 优化问题

$$\min_{\beta_0, \beta, \epsilon} \left\{\frac{1}{2}\|\beta\|^2 + C\sum_{i=1}^n \epsilon_i\right\}$$

约束条件：

$$y_i(\beta_0 + \beta^T x_i) \geq 1 - \epsilon_i, \quad \epsilon_i \geq 0, \quad \forall i$$

其中：
- $C$ 是惩罚参数
- $\epsilon_i$ 是松弛变量

## 9.3 支持向量机（SVM）与核方法

### 9.3.1 核函数
通过核函数将数据映射到高维空间：

$$f(x) = \beta_0 + \sum_{i=1}^n \alpha_i K(x, x_i)$$

常见核函数：

1. **线性核**：
   $$K(x_i,x_j) = \sum_{k=1}^p x_{ik}x_{jk}$$

2. **多项式核**：
   $$K(x_i,x_j) = (1 + \sum_{k=1}^p x_{ik}x_{jk})^d$$

3. **径向基核（RBF）**：
   $$K(x_i,x_j) = \exp(-\gamma\sum_{k=1}^p (x_{ik}-x_{jk})^2)$$

### 9.3.2 SVM对偶问题

$$\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_i y_j K(x_i,x_j)$$

约束条件：
$$0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0$$

## 9.4 多分类SVM

### 9.4.1 一对一（One-vs-One）
- 构建 $\frac{K(K-1)}{2}$ 个分类器
- 分类决策：投票法
- 优点：训练更快，每个分类器更平衡

### 9.4.2 一对多（One-vs-All）
- 构建 $K$ 个分类器
- 分类决策：选择最大决策值
- 优点：需要训练的模型更少

## 9.5 参数选择

### 9.5.1 重要参数

1. **惩罚参数 $C$**：
   - 大 $C$：强调减少误分类
   - 小 $C$：强调大间隔

2. **核函数参数**：
   - RBF核的 $\gamma$：控制影响范围
   - 多项式核的 $d$：控制非线性程度

### 9.5.2 网格搜索与交叉验证

$$CV(\lambda) = \frac{1}{n}\sum_{i=1}^n L(y_i, \hat{f}^{(-i)}(x_i; \lambda))$$

其中 $\hat{f}^{(-i)}$ 是不包含第 $i$ 个观测的模型。

## 9.6 SVM的损失函数

### 9.6.1 Hinge损失
$$L_{hinge}(y, f(x)) = \max(0, 1-yf(x))$$

### 9.6.2 比较
1. **Hinge损失（SVM）**：
   - 对正确分类且距离足够大的样本，损失为0
   - 线性惩罚错误分类

2. **对数损失（逻辑回归）**：
   $$L_{log}(y, f(x)) = \log(1 + e^{-yf(x)})$$
   - 连续平滑
   - 概率解释

## 9.7 优化算法

### 9.7.1 序列最小优化（SMO）
1. 选择两个拉格朗日乘子 $\alpha_i, \alpha_j$
2. 固定其他参数，优化这两个变量
3. 重复直到收敛

### 9.7.2 计算复杂度
- 时间复杂度：$O(n^2)$ 到 $O(n^3)$
- 空间复杂度：$O(n^2)$

## 9.8 实践考虑

### 9.8.1 数据预处理
1. **特征缩放**：
   $$x_{scaled} = \frac{x - \mu}{\sigma}$$

2. **缺失值处理**
3. **类别不平衡处理**：
   - 调整类别权重
   - 采样方法

### 9.8.2 模型选择流程
1. 数据预处理
2. 选择核函数
3. 参数网格搜索
4. 交叉验证评估
5. 最终模型训练

## 9.9 总结

### 9.9.1 SVM优势
1. 有效处理非线性问题
2. 理论基础扎实
3. 泛化能力强

### 9.9.2 局限性
1. 计算复杂度高
2. 参数调优复杂
3. 对大规模数据集不友好

### 9.9.3 适用场景
1. 中小规模数据集
2. 高维特征空间
3. 非线性分类问题
4. 需要高泛化性能的场景

