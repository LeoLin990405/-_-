# 第 6 章：线性模型选择与正则化

## 6.1 线性回归模型的背景与挑战

### 线性回归模型的定义
线性回归模型形式：

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon$$

其中：
- $\beta_0$：截距项
- $\beta_1, \dots, \beta_p$：回归系数
- $\epsilon$：随机误差项，$\epsilon \sim N(0, \sigma^2)$

### 挑战
1. **过拟合**：模型包含过多变量
2. **多重共线性**：预测变量高度相关
3. **高维数据**：$p$ 较大时系数估计不准确

## 6.2 变量选择方法

### 6.2.1 最优子集选择

#### 残差平方和计算
$$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

#### 信息准则

1. **AIC**：
$$AIC = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)$$

2. **BIC**：
$$BIC = \frac{1}{n}(RSS + \log(n)d\hat{\sigma}^2)$$

3. **Cp准则**：
$$C_p = \frac{RSS}{\hat{\sigma}^2} - n + 2d$$

4. **调整后的R方**：
$$R^2_{adj} = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$$

其中：
- $d$：模型中的变量数量
- $n$：样本量
- $TSS$：总平方和

## 6.3 正则化方法（Shrinkage Methods）

### 6.3.1 岭回归（Ridge Regression）

#### 目标函数
最小化带有L2惩罚项的目标函数：

$$\min_{\beta} \left\{\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2\right\}$$

等价于约束形式：

$$\min_{\beta} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \text{ subject to } \sum_{j=1}^p \beta_j^2 \leq t$$

其中：
- $\lambda \geq 0$：调优参数，控制惩罚强度
- $t$：系数大小的上界

### 6.3.2 Lasso回归（Lasso Regression）

#### 目标函数
最小化带有L1惩罚项的目标函数：

$$\min_{\beta} \left\{\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p |\beta_j|\right\}$$

等价于约束形式：

$$\min_{\beta} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \text{ subject to } \sum_{j=1}^p |\beta_j| \leq t$$

### 6.3.3 弹性网络（Elastic Net）

#### 目标函数
结合L1和L2惩罚：

$$\min_{\beta} \left\{\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda_1 \sum_{j=1}^p |\beta_j| + \lambda_2 \sum_{j=1}^p \beta_j^2\right\}$$

## 6.4 降维方法

### 6.4.1 主成分回归（PCR）

#### 步骤
1. 对预测变量进行主成分分析：
   $$Z_m = \sum_{j=1}^p \phi_{mj}X_j, \quad m=1,\dots,M$$

2. 用前M个主成分进行回归：
   $$Y = \theta_0 + \sum_{m=1}^M \theta_m Z_m + \epsilon$$

### 6.4.2 偏最小二乘回归（PLS）

#### 算法步骤
1. 标准化X和Y
2. 对m = 1到M：
   - 计算X与Y的相关性
   - 构造主成分方向
   - 正交化

## 6.5 模型评估和选择

### 6.5.1 交叉验证误差
使用k折交叉验证计算MSE：

$$CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i$$

### 6.5.2 正则化参数选择
通过网格搜索和交叉验证选择最优λ：

$$\lambda_{opt} = \arg\min_{\lambda} CV(\lambda)$$

## 6.6 比较与应用指南

### 6.6.1 方法比较
1. **岭回归**：
   - 适用于多重共线性
   - 所有变量都保留
   
2. **Lasso**：
   - 产生稀疏解
   - 自动进行变量选择
   
3. **PCR**：
   - 降维
   - 不考虑响应变量

4. **PLS**：
   - 降维
   - 考虑响应变量

### 6.6.2 实践建议
1. **变量选择**：
   - 小p：考虑最优子集选择
   - 大p：使用Lasso或前向逐步选择

2. **正则化选择**：
   - 需要变量选择：Lasso
   - 多重共线性：岭回归
   - 两者兼有：弹性网络

3. **降维选择**：
   - 预测变量高度相关：PCR
   - 需要考虑响应变量：PLS

## 6.7 总结

### 主要方法及特点
1. **变量选择**：
   - 最优子集
   - 逐步选择

2. **正则化**：
   - 岭回归（L2）
   - Lasso（L1）
   - 弹性网络（L1+L2）

3. **降维**：
   - PCR
   - PLS

### 应用注意事项
1. 数据预处理的重要性
2. 交叉验证的必要性
3. 计算效率的考虑
4. 模型解释性的权衡