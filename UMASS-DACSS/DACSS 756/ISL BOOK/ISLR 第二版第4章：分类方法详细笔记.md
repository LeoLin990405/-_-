# ISLR 第二版第4章：分类方法详细笔记

## 4.1 分类任务的概述

- 分类任务旨在根据输入变量 $X = (X_1, X_2, \dots, X_p)$ 预测响应变量 $Y$ 的类别
- **定性响应变量** $Y$ 可以取有限个类别标签：
  - 二分类：$Y \in \{0,1\}$
  - 多分类：$Y \in \{1,2,\dots,K\}$

## 4.2 线性回归为何不适用于分类

### 回归编码问题
假设有3个类别的医疗诊断：**中风**、**药物过量**和**癫痫发作**。若用数值编码为 $Y = \{1, 2, 3\}$，存在以下问题：

1. 类别之间不存在数值大小关系
2. 模型可能预测出超出类别范围的值
3. 类别间的距离关系没有实际意义：$2-1 = 1$，$3-2 = 1$

## 4.3 逻辑回归（Logistic Regression）

### 4.3.1 模型形式
对于二分类问题 $Y \in \{0, 1\}$，模型输出类别1的概率：

$$p(X) = \Pr(Y = 1 \mid X)$$

使用逻辑函数确保概率范围在 $[0,1]$ 之间：

$$p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}$$

### 4.3.2 对数几率变换
通过对数几率变换，将模型转化为线性形式：

$$\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X$$

### 4.3.3 最大似然估计

1. **似然函数**：
   $$L(\beta_0, \beta_1) = \prod_{i=1}^n p(x_i)^{y_i} \left(1 - p(x_i)\right)^{1-y_i}$$

2. **对数似然函数**：
   $$\ell(\beta_0, \beta_1) = \sum_{i=1}^n \left[ y_i \log p(x_i) + (1 - y_i) \log (1 - p(x_i)) \right]$$

3. **求解偏导**：
   $$\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \left( y_i - p(x_i) \right) x_{ij}$$

## 4.4 判别分析：LDA 和 QDA

### 4.4.1 线性判别分析（LDA）

#### 1. 模型假设
- 每个类别 $k$ 的样本服从多元正态分布：
  $$X \mid Y = k \sim N(\mu_k, \Sigma)$$
- $\mu_k$：类别 $k$ 的均值向量
- $\Sigma$：所有类别共享的协方差矩阵

#### 2. 密度函数
类别 $k$ 的密度函数：

$$f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)$$

#### 3. 后验概率
使用贝叶斯定理：

$$\Pr(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}$$

其中 $\pi_k$ 是类别 $k$ 的先验概率

#### 4. 判别函数
简化后的判别函数：

$$\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log(\pi_k)$$

### 4.4.2 二次判别分析（QDA）

#### 1. 模型假设
- 允许每个类别有不同的协方差矩阵：
  $$X \mid Y = k \sim N(\mu_k, \Sigma_k)$$

#### 2. 判别函数

$$\delta_k(x) = -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) - \frac{1}{2} \log |\Sigma_k| + \log(\pi_k)$$

#### 3. LDA与QDA比较
- LDA：线性决策边界，参数更少，适合小样本
- QDA：非线性决策边界，更灵活，需要更多数据

## 4.5 朴素贝叶斯（Naive Bayes）

### 4.5.1 核心假设
- 条件独立假设：给定类别时，特征之间相互独立
  $$f_k(x) = \prod_{j=1}^p f_{kj}(x_j)$$

### 4.5.2 后验概率

$$\Pr(Y = k \mid X = x) = \frac{\pi_k \prod_{j=1}^p f_{kj}(x_j)}{\sum_{l=1}^K \pi_l \prod_{j=1}^p f_{lj}(x_j)}$$

### 4.5.3 不同类型特征的处理
1. **连续变量**：通常假设正态分布
2. **离散变量**：使用相对频率估计概率
3. **缺失值**：可以忽略对应特征的贡献

## 4.6 分类模型的评估

### 4.6.1 评估指标
1. **混淆矩阵（Confusion Matrix）**：
   - 真正例（TP）
   - 假正例（FP）
   - 真负例（TN）
   - 假负例（FN）

2. **常用指标**：
   $$\text{准确率} = \frac{TP + TN}{TP + TN + FP + FN}$$
   $$\text{精确率} = \frac{TP}{TP + FP}$$
   $$\text{召回率} = \frac{TP}{TP + FN}$$
   $$\text{F1分数} = \frac{2 \cdot \text{精确率} \cdot \text{召回率}}{\text{精确率} + \text{召回率}}$$

### 4.6.2 ROC曲线与AUC
- ROC曲线：描绘不同阈值下的真正例率与假正例率关系
- AUC：ROC曲线下面积，衡量模型区分能力

## 4.7 模型选择建议

### 4.7.1 选择依据
1. **数据规模**：
   - 小样本：LDA优于QDA
   - 大样本：可以考虑更复杂的模型

2. **特征关系**：
   - 特征独立：朴素贝叶斯表现好
   - 特征相关：考虑LDA/QDA

3. **可解释性需求**：
   - 需要解释：逻辑回归
   - 仅需预测：可以选择更复杂模型

### 4.7.2 实践注意事项
1. 特征工程的重要性
2. 处理类别不平衡
3. 交叉验证的使用
4. 模型诊断和调优

## 总结
本章详细介绍了主要分类方法的数学原理和实际应用考虑：
- 逻辑回归：概率解释直观
- LDA/QDA：基于概率分布假设
- 朴素贝叶斯：计算效率高，适用于高维
- 每种方法都有其适用场景，需要根据具体问题选择合适的方法