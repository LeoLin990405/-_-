# 线性回归 (Linear Regression) 超详细笔记

---

## 1. 线性回归概述

线性回归是一种**统计学习方法**，用于描述**自变量**（预测变量）与**因变量**（响应变量）之间的关系。其核心思想是：找到一个数学模型，将输入变量（自变量）与输出变量（因变量）之间的关系尽可能精确地描述，并利用模型对未知数据进行预测。

### **线性回归的应用场景：**
1. **预测任务**：如广告支出预测销售额，房价预测等。
2. **解释任务**：了解哪些因素（如广告预算、市场条件）对响应变量（如销售额）有重要影响，以及影响的大小。
3. **模型推断**：检验变量之间的相关性与因果关系。

---

## 2. 简单线性回归 (Simple Linear Regression)

### 2.1 模型结构

**简单线性回归模型**描述一个**自变量**与一个**因变量**之间的线性关系。其数学表达如下：

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$

- **$Y$**：响应变量（因变量），如销售额、房价等。  
- **$X$**：预测变量（自变量），如广告预算、房屋面积等。  
- **$\beta_0$**：截距（Intercept），即当 $X = 0$ 时，响应变量 $Y$ 的预期值。  
- **$\beta_1$**：斜率（Slope），表示 $X$ 增加 1 个单位时，$Y$ 的平均变化量。  
- **$\epsilon$**：误差项（Noise），描述无法被 $X$ 解释的部分，包括随机波动和未观测到的变量。假设 $\epsilon \sim N(0, \sigma^2)$。

### 2.2 残差与最小二乘法 (Ordinary Least Squares)

**残差**是实际值与模型预测值之间的差异，即：

$$
e_i = y_i - \hat{y}_i
$$

为了使模型尽可能地拟合数据，我们采用**最小二乘法**（OLS），即最小化**残差平方和 (Residual Sum of Squares, RSS)**：

$$
RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
$$

通过**最小化 RSS**，我们可以求出最优的 $\beta_0$ 和 $\beta_1$：

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}
$$

其中，$\bar{x}$ 和 $\bar{y}$ 分别是自变量和因变量的均值。

---

### 2.3 模型评价

1. **残差标准误 (Residual Standard Error, RSE)**  
   RSE 衡量模型对数据拟合的准确程度，是残差的标准差：

   $$
   RSE = \sqrt{\frac{RSS}{n - 2}}
   $$

   RSE 越小，表示模型的预测误差越小。

2. **决定系数 $R^2$（R-Squared）**  
   $R^2$ 衡量模型对响应变量变异的解释能力：

   $$
   R^2 = 1 - \frac{RSS}{TSS}
   $$

   其中，$TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2$ 为总变异。$R^2$ 的取值范围为 0 到 1，越接近 1 说明模型解释的变异越多。

---

## 3. 多元线性回归 (Multiple Linear Regression)

### 3.1 模型结构

多元线性回归模型是简单线性回归的扩展，允许多个自变量：

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
$$

- **每个回归系数 $\beta_j$** 表示在控制其他变量不变的情况下，自变量 $X_j$ 每增加 1 个单位时，$Y$ 的平均变化。

---

### 3.2 系数估计与假设检验

1. **t 检验**：用于判断每个自变量是否对响应变量有显著影响。
   - **零假设**：$H_0: \beta_j = 0$，即 $X_j$ 对 $Y$ 无显著影响。
   - **t 统计量**：

     $$
     t_j = \frac{\hat{\beta_j}}{SE(\hat{\beta_j})}
     $$

     其中，$SE(\hat{\beta_j})$ 是 $\hat{\beta_j}$ 的标准误。

2. **p 值**：衡量观测到的结果在零假设为真时发生的概率。若 $p < 0.05$，则拒绝 $H_0$。

---

### 3.3 多重共线性 (Multicollinearity)

**定义**：当两个或多个自变量高度相关时，称为多重共线性。这会导致：
- **回归系数不稳定**：小的变化会导致系数估计发生较大波动。
- **方差膨胀**：标准误增加，t 检验失效。

**方差膨胀因子 (VIF)** 用于检测共线性：

$$
VIF(\beta_j) = \frac{1}{1 - R^2_{X_j | X_{-j}}}
$$

若 $VIF > 10$，则存在严重的多重共线性。

---

## 4. 扩展模型：交互作用与非线性关系

### 4.1 交互作用 (Interaction Effects)

当两个变量之间存在协同效应时，可以引入交互项：

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \times X_2) + \epsilon
$$

- **$\beta_3$** 表示 $X_1$ 和 $X_2$ 之间的交互效应。

---

### 4.2 非线性回归 (Nonlinear Regression)

对于非线性关系，我们可以通过**多项式回归**来拟合：

$$
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon
$$

这种方法将非线性关系转换为线性模型处理，方便使用线性回归技术。

---

## 5. 异常值与高杠杆点

1. **异常值 (Outliers)**  
   异常值是与预测值偏差较大的点，可能影响模型的拟合结果。

2. **高杠杆点 (High Leverage Points)**  
   高杠杆点是自变量取值极端的观测点，对模型系数有较大影响。

   **杠杆值**用于度量：

   $$
   h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n} (x_j - \bar{x})^2}
   $$

---

## 6. 线性回归与 KNN 的比较

### 6.1 K 近邻回归 (KNN Regression)

KNN 是一种非参数模型，不假设数据的分布。其基本思想是：对于每个测试样本，找到距离最近的 $K$ 个训练样本，并用这些样本的平均值作为预测。

---

### 6.2 线性回归与 KNN 的优劣

- **线性回归**：
  - 优点：简单易解释，适用于线性关系。
  - 缺点：在非线性关系下表现较差。

- **KNN**：
  - 优点：适用于非线性数据。
  - 缺点：在高维数据中性能下降（维度灾难）。

---

## 7. R 语言代码示例

```R
# 加载数据集
library(ISLR2)
data(Advertising)

# 拟合线性回归模型
lm_fit <- lm(sales ~ TV + radio + newspaper, data = Advertising)
summary(lm_fit)

# 绘制回归直线
plot(Advertising$TV, Advertising$sales)
abline(lm(sales ~ TV, data = Advertising), col = "blue")
```

---

## 8. 总结

线性回归是数据分析和机器学习中的基础模型，具有易于解释和实现的特点。然而，在实践中需要关注**异常值**、**多重共线性**等问题，并根据数据特性调整模型（如加入交互项或进行非线性变换）。

---

