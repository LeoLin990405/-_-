---

# 第二章 统计学习笔记

统计学习是一门通过数据来学习和推测变量间关系的领域，其核心是通过有限的数据样本来估计未知函数 $f$，以描述输入变量 $X$ 与输出变量 $Y$ 之间的关系。根据估计目标的不同，统计学习的任务可分为**预测**和**推断**两种。

---

## 1. 统计学习的基本任务

假设输出变量 $Y$ 和输入变量 $X = (X_1, X_2, \ldots, X_p)$ 之间存在如下关系：

$$
Y = f(X) + \epsilon
$$

其中：
- $f(X)$ 是一个未知的确定性函数，描述了 $X$ 与 $Y$ 的系统性关系；
- $\epsilon$ 是误差项，通常假设 $\epsilon \sim N(0, \sigma^2)$，即服从均值为零、方差为 $\sigma^2$ 的正态分布，反映了 $Y$ 的随机波动部分。

统计学习的任务是通过数据样本 $\{(X_i, Y_i)\}_{i=1}^n$ 来估计 $f$，实现以下两个主要目标：

### 1.1 预测（Prediction）

在预测任务中，我们关注的是通过已知的输入变量 $X$ 的值来预测未知的输出变量 $Y$。预测模型的目标是找到一个合适的 $\hat{f}$，使得我们可以利用 $X$ 预测 $Y$。其预测过程可表示为：

$$
\hat{Y} = \hat{f}(X)
$$

预测误差的来源主要分为两部分：
1. **可约误差（Reducible Error）**：由于 $\hat{f}$ 对 $f$ 的估计不够准确而导致的误差。这部分误差可以通过提高模型质量来减少。
2. **不可约误差（Irreducible Error）**：来自于 $\epsilon$ 的波动。由于 $\epsilon$ 反映了数据中不可预测的随机因素，即便完美地估计了 $f$，该误差仍然存在。

### 1.2 推断（Inference）

在推断任务中，我们的目标是理解 $X$ 和 $Y$ 之间的关系，着重于模型的可解释性。例如，回答以下问题：
- 哪些输入变量对 $Y$ 有显著影响？
- 每个变量对 $Y$ 的影响是正向还是负向？
- 是否存在复杂的非线性关系？

在推断任务中，模型的简洁性和可解释性比预测精度更为重要。

---

## 2. 估计 $f$ 的方法：参数法与非参数法

### 2.1 参数法（Parametric Methods）

**参数法**通过假设 $f$ 的特定函数形式来简化问题。通常的参数法分两步：
1. **设定模型形式**：假设 $f$ 的具体形式，如线性模型假设 $f$ 是 $X$ 的线性组合：
   $$
   f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p
   $$
   其中 $\beta_0, \beta_1, \dots, \beta_p$ 是待估计的参数。
2. **参数估计**：用数据估计出 $\beta$ 系数的值，使得模型尽可能拟合数据。

#### 最小二乘法（Least Squares）推导

给定样本点 $(x_i, y_i)$，我们希望找到参数 $\beta$ 使得模型预测值与真实值之间的残差平方和（Residual Sum of Squares, RSS）最小化。定义 RSS 为：

$$
\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip})\right)^2
$$

为了方便表示，令 $Y = X\beta + \epsilon$，其中：
- $Y = (y_1, y_2, \ldots, y_n)^T$ 是响应变量向量；
- $X$ 是 $n \times (p+1)$ 的设计矩阵，包含所有输入变量；
- $\beta = (\beta_0, \beta_1, \dots, \beta_p)^T$ 是回归系数向量。

则可以将 RSS 写成矩阵形式：

$$
\text{RSS} = (Y - X\beta)^T (Y - X\beta)
$$

对 $\beta$ 求导并令其导数为零，得到**正态方程（Normal Equations）**：

$$
X^T X \beta = X^T Y
$$

求解该方程可以得到 $\beta$ 的估计值：

$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$

**优缺点**：
- **优点**：通过假设模型形式来简化估计过程，计算效率高且易于解释。
- **缺点**：若假设模型形式与真实情况不符，可能会产生较大偏差（高偏差）。

### 2.2 非参数法（Non-Parametric Methods）

**非参数法**不对 $f$ 的形式做特定假设，而是通过数据直接构建 $f$。其目标是找到尽可能接近数据的函数形式，常用的非参数方法包括 **平滑样条（Splines）** 和 **K 最近邻（K-Nearest Neighbors, KNN）** 等。

#### 非参数法的优缺点

- **优点**：非参数法非常灵活，可以适应复杂的真实数据模式，不需要假设 $f$ 的形式。
- **缺点**：通常需要更多的数据样本来确保估计稳定；模型复杂性较高，解释难度较大。

---

## 3. 模型的灵活性与可解释性的权衡

模型的**灵活性（Flexibility）**与**可解释性（Interpretability）**之间通常存在权衡关系。灵活性高的模型（如平滑样条、深度学习）可以很好地拟合复杂的数据模式，但难以解释；而简单模型（如线性回归）易于解释，但可能无法捕捉复杂的关系。

### 灵活性与可解释性的比较

不同的统计学习方法在灵活性和可解释性上各有特点。例如：
- **线性回归（Linear Regression）**：假设 $f$ 是线性的，限制性强但易于理解。
- **广义加性模型（Generalized Additive Model, GAM）**：放宽了线性假设，允许一定程度的非线性。
- **决策树（Decision Trees）**：能够捕捉非线性关系，但易过拟合，灵活性较高，可解释性适中。
- **深度学习（Deep Learning）**：高度灵活，适合复杂的数据，但难以解释具体变量的作用。

---

## 4. 监督学习与非监督学习

根据是否有标签信息，统计学习可分为**监督学习**和**非监督学习**。

### 4.1 监督学习（Supervised Learning）

**监督学习**的数据包含输入变量和输出变量（标签），其目标是根据已知的标签来学习输入与输出的关系，从而对新数据进行预测。常见任务包括：
- **回归（Regression）**：用于预测连续变量。
- **分类（Classification）**：用于预测离散类别。

### 4.2 非监督学习（Unsupervised Learning）

**非监督学习**的数据仅包含输入变量，没有标签。目标是探索数据的内在结构。常见的非监督学习任务有：
- **聚类（Clustering）**：将数据分组，使同一组内的数据相似，不同组的数据差异较大。
- **降维（Dimensionality Reduction）**：在保留数据信息的前提下减少数据维度，例如主成分分析（PCA）。

### 4.3 半监督学习（Semi-Supervised Learning）

**半监督学习**结合了监督和非监督学习，部分数据带有标签，部分数据无标签。这种方法在标签获取成本较高的情况下具有应用价值。

---

## 5. 回归与分类问题

### 回归问题

**回归问题（Regression）**的输出变量为连续值（如温度、价格），模型目标是找到输入和输出之间的函数关系，便于对新数据进行预测。

### 分类问题

**分类问题（Classification）**的输出变量为类别标签（如是否患病）。分类模型的目标是将输入变量分配到不同的类别中。

**错误率（Error Rate）**是衡量

分类模型准确性的指标之一。假设预测标签为 $\hat{y}_i$，真实标签为 $y_i$，则错误率定义为：

$$
\text{Error Rate} = \frac{1}{n} \sum_{i=1}^n I(y_i \ne \hat{y}_i)
$$

其中 $I(y_i \ne \hat{y}_i)$ 为指示函数，当 $y_i \ne \hat{y}_i$ 时取 1，否则为 0。

---

## 6. 模型评估：训练误差与测试误差

在回归任务中，常用**均方误差（Mean Squared Error, MSE）**来衡量模型的精度：

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{f}(x_i))^2
$$

其中 $y_i$ 是真实值，$\hat{f}(x_i)$ 是模型预测值。根据数据的来源，MSE 可分为：
- **训练误差（Training Error）**：在训练数据上计算得到的 MSE，反映了模型对已知数据的拟合程度。
- **测试误差（Test Error）**：在新数据上计算得到的 MSE，测试误差能更好地反映模型的泛化能力。

模型的实际效果往往通过测试误差来衡量。

---

## 7. 偏差-方差分解（Bias-Variance Trade-Off）

预测误差的期望可以分解为偏差平方、方差和不可约误差的和：

$$
\text{Expected Test Error} = \text{Bias}(\hat{f})^2 + \text{Var}(\hat{f}) + \sigma^2
$$

其中：
- **偏差（Bias）**：模型与真实关系 $f$ 的偏离程度。简单模型（如线性模型）通常偏差较高。
- **方差（Variance）**：模型对数据波动的敏感性。复杂模型（如深度学习）对数据敏感，方差较大。
- **不可约误差** $\sigma^2$：来源于 $\epsilon$ 的波动，无法通过改进模型降低。

偏差与方差的权衡关系是选择模型的关键，通常我们需要在低偏差和低方差之间找到平衡。

---

## 8. K 最近邻（K-Nearest Neighbors, KNN）分类器

KNN 是一种常用的非参数分类方法。给定新的样本 $x_0$，KNN 分类步骤如下：
1. 选择距离 $x_0$ 最近的 $K$ 个训练样本。
2. 统计这些样本的类别频率，将出现最多的类别作为 $x_0$ 的预测类别。

### K 值选择的影响

- **较小的 $K$ 值**：模型过拟合，误差方差大，容易受噪声影响。
- **较大的 $K$ 值**：模型欠拟合，误差偏差大，不能捕捉数据细节。

KNN 的预测效果受 $K$ 值影响，因此选择合适的 $K$ 值至关重要。