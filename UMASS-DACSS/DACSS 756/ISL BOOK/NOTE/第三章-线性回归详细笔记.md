### 线性回归详细笔记

线性回归是分析因变量与一个或多个自变量之间关系的统计方法，通常用于量化预测。我们将首先介绍简单线性回归，包括模型定义、参数估计、模型评价和假设检验；接着介绍多元线性回归，并提供模型评价的详细方法和推导过程，最后讨论线性回归的假设条件和局限性。

---

### 1. 简单线性回归

#### 1.1 模型定义
简单线性回归用于研究一个因变量 $Y$ 和一个自变量 $X$ 之间的线性关系。其模型定义为：
$$
Y = \beta_0 + \beta_1 X + \epsilon
$$
其中：
- $Y$：因变量（响应变量），我们希望预测的变量；
- $X$：自变量（预测变量），用于解释或预测 $Y$ 的变化；
- $\beta_0$：截距，表示当 $X=0$ 时 $Y$ 的预测值；
- $\beta_1$：斜率，表示 $X$ 增加一个单位时 $Y$ 的平均增量；
- $\epsilon$：误差项，反映了 $Y$ 中未被 $X$ 解释的变异部分，通常假设 $\epsilon \sim N(0, \sigma^2)$。

#### 1.2 最小二乘法估计的推导
为了估计模型中的未知参数 $\beta_0$ 和 $\beta_1$，我们使用**最小二乘法**。最小二乘法的目标是最小化预测值与真实值之间的差异平方和，即最小化残差平方和（Residual Sum of Squares, RSS）。定义如下：
$$
\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2
$$

##### 1.2.1 对 $\beta_0$ 的偏导数
首先对 $\beta_0$ 求偏导数并设为 $0$，以找到最小值点：
$$
\frac{\partial \text{RSS}}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
$$
展开偏导：
$$
= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)
$$
设 $\frac{\partial \text{RSS}}{\partial \beta_0} = 0$，得到：
$$
\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) = 0
$$
整理后得：
$$
n \beta_0 = \sum_{i=1}^n y_i - \beta_1 \sum_{i=1}^n x_i
$$
即：
$$
\beta_0 = \bar{y} - \beta_1 \bar{x}
$$
其中 $\bar{x}$ 和 $\bar{y}$ 分别为 $X$ 和 $Y$ 的均值，定义为 $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ 和 $\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$。

##### 1.2.2 对 $\beta_1$ 的偏导数
接下来对 $\beta_1$ 求偏导数并设为 $0$：
$$
\frac{\partial \text{RSS}}{\partial \beta_1} = \frac{\partial}{\partial \beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
$$
展开偏导：
$$
= -2 \sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i)
$$
代入 $\beta_0 = \bar{y} - \beta_1 \bar{x}$ 得：
$$
\sum_{i=1}^n x_i (y_i - \bar{y}) = \beta_1 \sum_{i=1}^n x_i (x_i - \bar{x})
$$
因此，$\beta_1$ 的估计值为：
$$
\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
$$

#### 1.3 模型评价

##### 1.3.1 残差标准误差（RSE）的推导
残差标准误差（RSE）用于衡量模型对数据的拟合程度，它是误差项 $\epsilon$ 的标准差估计。定义为：
$$
\text{RSE} = \sqrt{\frac{1}{n - 2} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
$$
其中 $\hat{y}_i$ 是模型的预测值，$n-2$ 是自由度调整项（因为估计了两个参数$\beta_0$ 和 $\beta_1$）。

##### 1.3.2 $R^2$统计量的推导
$R^2$ 表示回归模型对因变量变异的解释比例，其定义为：
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$
其中，总平方和（TSS）定义为：
$$
\text{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2
$$
$R^2$ 的取值范围为 $[0,1]$，其值越接近 1 表明模型的解释力越强。

#### 1.4 假设检验

在简单线性回归中，我们通常希望检验 $X$ 和 $Y$ 之间是否存在显著的线性关系。为此，可以检验斜率 $\beta_1$ 是否显著不为零。

##### 1.4.1 $t$ 检验
为了检验 $\beta_1$ 是否显著非零，我们设定假设：
- **原假设** $H_0$：$\beta_1 = 0$（没有线性关系）；
- **备择假设** $H_a$：$\beta_1 \neq 0$（存在线性关系）。

$t$ 检验的统计量定义为：
$$
t = \frac{\hat{\beta_1}}{\text{SE}(\hat{\beta_1})}
$$
其中 $\text{SE}(\hat{\beta_1})$ 为 $\beta_1$ 的标准误差，定义为：
$$
\text{SE}(\hat{\beta_1}) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}
$$
当 $t$ 值较大（即 $p$ 值较小）时，可以拒绝原假设 $H_0$，认为 $X$ 与 $Y$ 之间存在显著关系。

##### 1.4.2 置信区间
在假设 $\beta_1$ 的估计值 $\hat{\beta_1}$ 服从正态分布的情况下，我们可以计算出 $\beta_1$ 的 $95\%$ 置信区间为：
$$
\hat{\beta_1} \pm t_{n-2,0.975} \cdot \text{SE}(\hat{\beta_1})
$$
其中 $t_{n-2,0.975}$ 是自由度为 $n-2$ 的 $t$ 分布的 $97.5\%$ 分位数。

---

### 2. 多元线性回归

多元线性回归用于描述一个因变量 $Y$ 与多个自变量 $X_1, X_2, \dots, X_p$ 之间的关系。多元线性回归的模型形式为：
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon
$$
其中 $\beta_j$ 表示在控制其他变量不变的情况下，自变量 $X_j$ 对 $Y$ 的平均影响。

#### 2.1 多元线性回归的最小二乘估计

多元回归的最小二乘估计与简单线性回归类似，目标是最小化残差平方和：
$$
\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \left(y_i - \left(\beta_0 + \sum

_{j=1}^p \beta_j x_{ij}\right)\right)^2
$$

为方便表述，将公式写成矩阵形式：
$$
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$
其中：
- $\mathbf{Y}$ 为 $n \times 1$ 的响应向量；
- $\mathbf{X}$ 为 $n \times (p+1)$ 的设计矩阵，其中第一列为全1，用于表示截距项；
- $\boldsymbol{\beta}$ 为 $(p+1) \times 1$ 的参数向量；
- $\boldsymbol{\epsilon}$ 为 $n \times 1$ 的误差向量。

最小二乘估计的解为：
$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
$$

#### 2.2 假设检验：F检验

为了检验模型整体显著性，我们可以进行 F 检验，检验是否至少有一个自变量对因变量有显著影响。

- **原假设** $H_0$：$\beta_1 = \beta_2 = \dots = \beta_p = 0$
- **备择假设** $H_a$：至少存在一个 $\beta_j \neq 0$

F 统计量的计算公式为：
$$
F = \frac{\left(\text{TSS} - \text{RSS}\right) / p}{\text{RSS} / (n - p - 1)}
$$
其中：
- $\text{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2$ 是总平方和；
- $\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$ 是残差平方和。

若 $F$ 值显著大于 1，且相应的 $p$ 值小于设定的显著性水平，则拒绝原假设 $H_0$，认为至少有一个预测变量与因变量显著相关。

---

### 3. 模型评估

在多元线性回归中，常用以下指标来评估模型的拟合效果：

#### 3.1 残差标准误差（RSE）
$$
\text{RSE} = \sqrt{\frac{1}{n - p - 1} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
$$
RSE 越小表示模型拟合越好。

#### 3.2 $R^2$ 统计量
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
$$
$R^2$ 越接近 1，表明模型对因变量的解释力越强。

#### 3.3 调整后的 $R^2$
调整后的 $R^2$ 通过惩罚模型复杂度，避免模型因引入更多变量而虚高，定义为：
$$
R^2_{\text{adj}} = 1 - \frac{\text{RSS} / (n - p - 1)}{\text{TSS} / (n - 1)}
$$

---

### 4. 模型的假设条件和局限性

线性回归模型的假设包括：
1. **线性关系**：因变量和自变量之间存在线性关系。
2. **独立性**：误差项彼此独立。
3. **正态性**：误差项 $\epsilon$ 服从正态分布。
4. **同方差性**：误差项的方差相同。

当数据不满足这些假设时，可能需要其他模型或进行数据变换（如对数变换或多项式回归）来改善模型拟合效果。

---

### 总结

通过详细的公式推导，我们可以清晰地理解线性回归模型中各参数的计算方法、假设检验过程以及模型的评估指标。在统计学习中，线性回归是其他复杂模型的基础，对其深入理解将有助于学习更为复杂的统计建模方法。