# ISLR 第8章：基于树的模型（Tree-Based Methods）

## 8.1 决策树的基础

### 8.1.1 决策树概述
- **决策树**是一种非参数监督学习方法，适用于回归和分类任务
- 通过递归分割特征空间，每个区域的预测基于：
  - 回归：区域内样本的均值
  - 分类：区域内样本的众数

### 8.1.2 决策树的结构
1. **根节点**：包含所有训练数据
2. **内部节点**：基于特征进行分割 
3. **叶节点**：包含最终预测结果
4. **路径**：从根到叶的决策序列

### 8.1.3 回归树

#### 构建过程：递归二叉分割
目标：最小化每个区域内的RSS：

$$RSS = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$$

其中：
- $R_j$ 是第 $j$ 个区域
- $\hat{y}_{R_j}$ 是区域 $R_j$ 中样本的响应均值

### 8.1.4 复杂度剪枝
带惩罚项的损失函数：

$$C_\alpha(T) = \sum_{m=1}^{|T|} \sum_{i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|$$

其中：
- $|T|$ 是叶节点数量
- $\alpha$ 是复杂度参数，控制树的大小

### 8.1.5 分类树
分类树的不纯度度量：

1. **分类错误率**：
   $$E = 1 - \max_k \hat{p}_{mk}$$

2. **Gini指数**：
   $$G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$$

3. **熵**：
   $$H = -\sum_{k=1}^K \hat{p}_{mk}\log(\hat{p}_{mk})$$

其中 $\hat{p}_{mk}$ 是节点 $m$ 中第 $k$ 类的样本比例

## 8.2 集成学习方法

### 8.2.1 Bagging（Bootstrap Aggregating）

#### 基本原理
1. 从训练集生成B个bootstrap样本
2. 对每个样本训练一个完整决策树
3. 预测时进行组合：
   - 回归：平均值
   $$\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B \hat{f}_b(x)$$
   
   - 分类：多数投票
   $$\hat{C}_{bag}(x) = \text{mode}\{\hat{C}_b(x)\}_{1}^B$$

#### OOB（Out-of-Bag）误差估计
$$\text{OOB Error} = \frac{1}{n}\sum_{i=1}^n L(y_i, \hat{y}_i^{OOB})$$

其中 $\hat{y}_i^{OOB}$ 是仅使用未抽中该样本的树进行预测。

### 8.2.2 随机森林（Random Forests）

#### 算法步骤
1. 对 $b = 1,\dots,B$:
   - 生成bootstrap样本
   - 在每个分裂点随机选择 $m$ 个特征
   - 生成树 $T_b$

2. 输出预测：
   - 回归：$$\hat{f}_{rf}(x) = \frac{1}{B}\sum_{b=1}^B T_b(x)$$
   - 分类：$$\hat{C}_{rf}(x) = \text{mode}\{T_b(x)\}_{1}^B$$

#### 特征重要性
$$\text{VI}_j = \frac{1}{B}\sum_{b=1}^B (\text{OOB MSE}_{b,\pi j} - \text{OOB MSE}_b)$$

其中 $\text{OOB MSE}_{b,\pi j}$ 是特征 $j$ 随机置换后的OOB误差。

### 8.2.3 Boosting

#### 梯度提升树（Gradient Boosting Trees）

1. 初始化 $f_0(x) = 0$

2. 对 $m = 1,\dots,M$:
   $$f_m(x) = f_{m-1}(x) + \lambda\gamma_m h_m(x)$$
   
   其中：
   - $h_m(x)$ 是第 $m$ 个基学习器
   - $\lambda$ 是学习率
   - $\gamma_m$ 是步长

3. 最终模型：
   $$f_M(x) = \sum_{m=1}^M \lambda\gamma_m h_m(x)$$

#### XGBoost优化目标
$$\text{obj}^{(t)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)$$

其中：
- $l$ 是损失函数
- $\Omega$ 是正则化项
- $f_t$ 是第 $t$ 轮的树模型

### 8.2.4 BART（Bayesian Additive Regression Trees）

#### 模型形式
$$Y = \sum_{j=1}^m g(X; T_j, M_j) + \epsilon, \quad \epsilon \sim N(0, \sigma^2)$$

其中：
- $T_j$ 是树结构
- $M_j$ 是叶节点参数
- $g(X; T_j, M_j)$ 是单棵树的预测

## 8.3 实践建议

### 8.3.1 方法选择
1. **单棵树**：
   - 优点：可解释性强
   - 缺点：预测准确率较低

2. **随机森林**：
   - 优点：稳定性好，无需调参
   - 缺点：计算开销大

3. **Boosting**：
   - 优点：预测准确率高
   - 缺点：调参复杂，易过拟合

### 8.3.2 参数设置
1. **树深度**：控制模型复杂度
2. **最小叶节点样本数**：防止过拟合
3. **特征采样比例**：增加树的多样性
4. **学习率**：影响模型收敛速度

## 8.4 总结

### 8.4.1 优缺点比较
| 方法     | 优点       | 缺点     |
| -------- | ---------- | -------- |
| 决策树   | 可解释性强 | 不稳定   |
| 随机森林 | 稳定性好   | 计算量大 |
| Boosting | 精度高     | 调参困难 |
| BART     | 自动调参   | 计算复杂 |

### 8.4.2 应用场景
- 特征选择：使用随机森林的重要性度量
- 高维数据：集成方法表现优秀
- 非线性关系：树模型天然处理非线性
- 缺失值处理：树模型具有内置处理机制