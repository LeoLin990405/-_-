# **分类模型与评价方法超详细笔记（研究生级别）**

---

## **一、分类问题概述**

### 1.1 分类与回归的区别  

- **回归**：用于预测**连续变量**，目标是拟合自变量 \(X = (X_1, X_2, \dots, X_p)\) 与因变量 \(Y \in \mathbb{R}\) 之间的关系：
  $  
  Y = f(X) + \epsilon  
  $
  其中 $ \epsilon $ 是误差项。常用回归模型包括**线性回归**、**多项式回归**、**岭回归**等。

- **分类**：用于预测**离散标签变量**，输出是离散类别标签，如 $ Y \in \{0, 1\} $ 或 $ Y \in \{A, B, C\} $。分类模型的目标是找出自变量 $X$ 和标签 $Y$ 之间的映射关系。分类模型既可以输出类别标签，也可以输出每个类别的概率。

---

## **二、Logistic回归：公式推导与详解**

### 2.1 二分类问题的描述

假设目标变量 $Y \in \{0, 1\}$，我们希望根据特征 $X = (X_1, X_2, ..., X_p)$ 预测其标签。

- **正类 (1)**：代表事件发生（如邮件为垃圾邮件）。
- **负类 (0)**：代表事件未发生（如邮件不是垃圾邮件）。

目标是找到一个模型，使得给定输入特征 $X$ 时，模型能够输出**标签为1的概率**：
$$
\pi(X) = P(Y = 1 | X)  
$$
同时：
$$
P(Y = 0 | X) = 1 - \pi(X)  
$$

---

### 2.2 **Logistic回归模型的构建**

我们假设 $X$ 与 $Y$ 之间有如下**线性关系**：
$  
\eta(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p  
$
但是，$\eta(X)$ 的值在 $(-\infty, +\infty)$ 范围内，不适合作为概率，因此需要使用**Sigmoid函数**将其映射到 $[0, 1]$ 区间。

---

### 2.3 **Sigmoid函数**

Sigmoid函数（也称为**逻辑函数**）定义为：
$$
\pi(X) = \frac{1}{1 + e^{-\eta(X)}}  
$$
其中：
$  
\eta(X) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p  
$

Sigmoid函数将线性模型的输出转化为**概率**，并具有以下性质：
- 当 $\eta(X) \to +\infty$，$\pi(X) \to 1$；
- 当 $\eta(X) \to -\infty$，$\pi(X) \to 0$。

---

### 2.4 **对数几率（Log Odds）与线性模型的联系**

Logistic回归本质上拟合的是事件发生的**对数几率（log odds）**：
$$
\text{logit}(\pi(X)) = \ln\left(\frac{\pi(X)}{1 - \pi(X)}\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p  
$$
- **对数几率**将概率比 $ \frac{\pi(X)}{1 - \pi(X)} $ 转化为实数，以便用线性模型拟合。

---

### 2.5 **概率的计算公式**

通过将线性模型带入Sigmoid函数，可以得到事件 $Y = 1$ 的概率：
$$
\pi(X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)}}  
$$

---

### 2.6 **Logistic回归的系数解释**

- **系数 $ \beta_i $**：表示特征 $X_i$ 每增加1个单位，对数几率 $ \ln\left(\frac{\pi(X)}{1 - \pi(X)}\right) $ 的变化量为 $ \beta_i $。
- **正系数**：意味着该特征的增加提高了事件 $Y = 1$ 的概率。
- **负系数**：意味着该特征的增加降低了事件 $Y = 1$ 的概率。

---

### 2.7 **Logistic回归的极大似然估计**

Logistic回归的参数 $\beta = (\beta_0, \beta_1, ..., \beta_p)$ 是通过**极大似然估计（MLE）**得到的。

#### 1. **似然函数**：
假设我们有 $n$ 个样本 $ (X_i, Y_i) $，每个样本的输出 $Y_i$ 是0或1，则每个样本的条件概率为：
$$
P(Y_i | X_i) = \pi(X_i)^{Y_i} \cdot (1 - \pi(X_i))^{1 - Y_i}  
$$
整个数据集的**似然函数**为：
$$
L(\beta) = \prod_{i=1}^{n} \pi(X_i)^{Y_i} \cdot (1 - \pi(X_i))^{1 - Y_i}  
$$

#### 2. **对数似然函数**：
为简化计算，我们取对数得到**对数似然函数**：
$$
\ln L(\beta) = \sum_{i=1}^{n} \left[ Y_i \ln \pi(X_i) + (1 - Y_i) \ln (1 - \pi(X_i)) \right]  
$$
通过最大化这个对数似然函数，可以得到参数估计值 $\hat{\beta}$。

---

## **三、判别分析（LDA 和 QDA）**

### 3.1 线性判别分析（LDA）

LDA 假设每个类别的数据服从**多元正态分布**，且类别之间的**协方差矩阵相同**。

1. **类别 $k$ 的条件概率密度**：
$$
P(X | Y = k) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (X - \mu_k)^T \Sigma^{-1} (X - \mu_k) \right)  
$$

2. **LDA判别函数**：
$$
\delta_k(X) = X^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \ln P(Y = k)  
$$

3. **LDA的决策边界**：为线性决策边界。

---

### 3.2 二次判别分析（QDA）

QDA 与 LDA 类似，但**允许每个类别有不同的协方差矩阵**，因此决策边界是**非线性**的。

---

## **四、评价指标与混淆矩阵**

### 4.1 混淆矩阵（Confusion Matrix）

| **预测/真实** | **正类 (1)** | **负类 (0)** |
| ------------- | ------------ | ------------ |
| **正类 (1)**  | TP (真正例)  | FP (假正例)  |
| **负类 (0)**  | FN (假负例)  | TN (真负例)  |

- **TP**：真正例，预测为正且实际为正。
- **FP**：假正例，预测为正但实际为负。
- **FN**：假负例，预测为负但实际为正。
- **TN**：真负例，预测为负且实际为负。

---

### 4.2 常用评价指标

1. **准确率（Accuracy）**：
$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}  
$$

2. **精确率（Precision）**：
$$
\text{Precision} = \frac{TP}{TP + FP}  
$$

3. **召回率（Recall）**：
$$
\text{Recall} = \frac{TP}{TP + FN}  
$$

4. **F1 分数（F1 Score）**：
$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}  
$$

5. **ROC曲线与AUC**：
   - **ROC曲线**展示不同阈值下的**真正例率**（TPR）和**假正例率**（FPR）。
   - **AUC（Area Under Curve）**：ROC曲线下的面积，越接近1表示模型越好。

---

## **五、Python 代码实现**

```python
import pandas as pd


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 加载数据
data = pd.read_csv("https://path_to_data.csv")
X = data[['gre', 'gpa', 'rank']]
y = data['admit']

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 训练Logistic回归模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# 计算混淆矩阵与AUC
cm = confusion_matrix(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print("Confusion Matrix:\n", cm)
print(f"AUC: {auc:.2f}")

# 绘制ROC曲线
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
```

---

## **六、总结**

- **Logistic回归**适用于二分类问题，能够输出概率并通过Sigmoid函数保证输出在 $[0, 1]$ 之间。
- **LDA** 假设线性决策边界，而 **QDA** 允许非线性决策边界。
- **模型评价**需要结合混淆矩阵、AUC 等多种指标进行全面评估。