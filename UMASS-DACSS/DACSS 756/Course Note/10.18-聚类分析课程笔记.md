# 聚类分析课程笔记

---

## 1. 课程安排与重要通知
- **时间安排**：
  - **本周四（10月10日）**：讲者因婚礼安排，本周四无课堂授课，但会发布类似上周PCA活动的练习，学生可以在教室或通过Zoom完成。
  - **下周二（10月15日）**：无课。10月14日是“土著人民日”（Indigenous Peoples Day），校方安排当周的周二按周一课程表执行，因此该课程停课。
- **作业安排**：
  - 第一份作业的反馈预计会在第二份作业截止前完成。

---

## 2. 课程主题概览
- **学习单元**：无监督学习
  - **上周主题**：主成分分析（PCA），一种用于数据降维的无监督学习方法。
  - **本周主题**：聚类分析（Clustering），另一种无监督学习方法，主要用于数据集的分组和分类。

---

## 3. 聚类分析概述
- **聚类分析**是一种无监督学习方法，目标是将数据集划分为若干个互不重叠的子集（即“簇”），使得同簇内的数据点之间的相似度最大，而不同簇之间的相似度最小。
- **主要应用**：
  - 市场细分：识别不同的客户群体。
  - 图像分割：将图像划分为不同区域。
  - 基因表达分析：在生物信息学中用于识别基因的分组。
  - 文本分析：在自然语言处理中的文本聚类。

---

## 4. 常见的聚类方法

### 4.1 K均值聚类（K-means Clustering）

#### 1. 基本概念
- K均值聚类是一种基于距离的分割聚类方法（partitioning clustering），目标是通过最小化簇内方差（within-cluster variance）将数据点分为 $K$ 个聚类。
- 适用于数值型数据，且在数据集较大、维度较低的情况下具有较高的计算效率。

#### 2. 优化目标
- **簇内方差**定义为一个簇中所有数据点与该簇中心点之间的距离平方和。
- **K均值聚类的优化目标**是找到最佳的聚类方案，使得所有簇的簇内方差之和最小。目标函数如下：
  $$
  \min_{C_1, C_2, \dots, C_K} \sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2
  $$
  其中：
  - $x_i$ 表示数据点 $i$，
  - $\mu_k$ 是簇 $C_k$ 的质心（即簇内所有数据点的均值），
  - $C_k$ 表示第 $k$ 个簇。

#### 3. 算法步骤
- **步骤 1：初始化** - 随机选择 $K$ 个初始中心点（或随机分配每个点到某个簇），这是K均值聚类的起始状态。
- **步骤 2：计算簇中心** - 对每个簇 $C_k$，计算其中心（均值向量），公式为：
  $$
  \mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
  $$
  其中 $|C_k|$ 表示簇 $C_k$ 中的数据点数。
- **步骤 3：重新分配** - 将每个数据点分配到最近的簇中心（通过最小化欧式距离）：
  $$
  C_k = \{x_i : \|x_i - \mu_k\|^2 \leq \|x_i - \mu_j\|^2, \forall j \neq k\}
  $$
- **步骤 4：检查收敛** - 如果簇内数据点的分配不再变化，算法收敛；否则返回步骤 2 和 3 继续迭代。

#### 4. 距离度量
- K均值聚类通常使用**欧氏距离**（Euclidean Distance）来衡量数据点之间的距离。欧氏距离定义为：
  $$
  d(x_i, x_j) = \sqrt{\sum_{p=1}^{P} (x_{i, p} - x_{j, p})^2}
  $$
  其中 $x_{i, p}$ 和 $x_{j, p}$ 是数据点 $x_i$ 和 $x_j$ 在第 $p$ 个特征上的值。

#### 5. 收敛性和局部最优问题
- **收敛性**：K均值算法在每次迭代中都会减少簇内方差，因此算法一定会收敛，但可能会陷入局部最优。
- **多次随机初始化**：为避免局部最优解，通常会进行多次随机初始化，每次都重新随机选择初始簇中心，最终选择最小化簇内方差的解。

#### 6. K值的选择
- K均值聚类要求用户事先指定聚类数量 $K$，而最优的 $K$ 值取决于数据的分布。**肘部法则**（Elbow Method）是确定 $K$ 值的常用方法。
- 该方法的具体步骤：
  - 计算不同 $K$ 值下的簇内方差和（Sum of Within-Cluster Variation, WCV）。
  - 绘制 $K$ 值与 WCV 的关系图，通常可以看到曲线在某个点急剧减小后趋于平缓，该点即为推荐的 $K$ 值。

#### 7. K均值聚类的优缺点
- **优点**：算法简单、计算效率高，适合大规模数据。
- **缺点**：需要事先指定 $K$ 值；对初始点敏感，可能陷入局部最优解；仅适用于数值型数据，且不适合非凸形状的聚类。

---

### 4.2 层次聚类（Hierarchical Clustering）

#### 1. 基本概念
- 层次聚类是一种基于树状结构的聚类方法，通过逐步合并或拆分数据点，构建数据的层次结构。
- 层次聚类可分为**凝聚层次聚类**（自底向上）和**分裂层次聚类**（自顶向下）。本课程主要讨论凝聚层次聚类。

#### 2. 算法步骤
- **初始状态**：将每个数据点视为单独的一个簇。
- **迭代合并**：每次找到两个距离最近的簇，并将其合并成一个簇。
- **终止条件**：重复以上步骤，直到所有数据点合并成一个簇，或达到指定的聚类数量。

#### 3. 距离度量和链接方式
- 层次聚类的效果取决于选择的距离度量和链接方式。常见的距离度量包括欧氏距离和相关性距离。常见的链接方式包括：
  - **单链接（Single Linkage）**：簇间最小距离，即簇 $A$ 和簇 $B$ 中任意数据点之间的最小距离：
    $$
    d(A, B) = \min_{x \in A, y \in B} \|x - y\|
    $$
  - **完全链接（Complete Linkage）**：簇间最大距离，即簇 $A$ 和簇 $B$ 中任意数据点之间的最大距离：
    $$
    d(A, B) = \max_{x \in A, y \in B} \|x - y\|
    $$
  - **平均链接（Average Linkage）**：簇间所有数据点距离的平均值：
    $$
    d(A, B) = \frac{1}{|A||B|} \sum_{x \in A, y \in B} \|x - y\|
    $$
  - **质心链接（Centroid Linkage）**：簇 $A$ 和簇 $B$ 的质心间距离：
    $$
    d(A, B) = \|\mu_A - \mu_B\|
    $$
    其中 $\mu_A$ 和 $\mu_B$ 分别表示簇 $A$ 和簇 $B$ 的质心。

#### 4. 层次聚类的优缺点
- **优点**：无需预先指定聚类数量 $K$；层次聚类算法能够发现数据的层次结构，适用于较小数据集。
- **缺点**：对噪声数据敏感，计算复杂度高，不适合大规模数据；不同的距离度量和链接方式可能导致显著不同的聚类结果。

---

## 5. 聚类分析中的常见问题

1. **特征缩放**：聚类算法对特征的尺度非常敏感，通常需要对数据进行标准化处理（如Z-score标准化或Min-Max归一化）以确保所有特征具有相似的尺度。
2. **相似性度量的选择**：选择合适的相似性度量（如欧氏距离、曼哈顿距离或相关性距离）至关重要，它将直接影响聚类效果。
3. **簇数量的选择**：
   - K均值聚类需要预先指定聚类数量 $K$，可通过肘部法则等方法选择最佳 $K$。
   - 层次聚类无需指定 $K$，但实际应用中仍需通过“树状图切割”来确定聚类数量。
4. **聚类结果的解释**：聚类结果需结合领域知识和实际需求进行解释和调整，特别是当结果不符合预期时，需要重新评估特征选择和相似性度量的合理性。

---

## 6. 实际应用案例

### 案例1：遗产语言学习者的聚类分析
- **背景**：研究遗产语言学习者的不同类型。
- **方法**：使用K均值聚类，基于多项语言习得特征对学习者进行分组。
- **输入特征**：
  - **习得年龄**：学习遗产语言与英语的年龄差异。
  - **语言熟练度**：遗产语言和英语的熟练程度。
  - **家庭语言暴露**：在家庭中接触遗产语言的频率。
  - **文化与家庭联系**：与遗产文化和家庭的联系程度。
  - **语言维护努力**：在遗产语言上的维护和学习投入。
- **结果解读**：基于K均值聚类，将学习者分为三类，如同时习得遗产语言和英语、先学遗产语言后学英语等类型。

### 案例2：基于回访数据的质量评估
- **背景**：利用回访数据评估问卷数据的质量。
- **方法**：使用多项式分布的混合模型生成一致性指标（agreement vectors），将数据分为高质量和低质量两类。
- **步骤**：
  1. 生成每个数据点的“匹配向量”（agreement vector），标记为完全一致、部分一致或不一致。
  2. 统计每个数据点的不同一致性类型的数量，形成“总结向量”（summary vector）。
  3. 使用混合模型对数据聚类，分为高质量与低质量数据集。
- **优点**：相比K均值，参数模型聚类方法能为每个数据点提供高质量概率，具备更高的灵活性和解释性。

---

## 7. 聚类方法的选择和应用建议

- **K均值聚类**：适用于数值型数据、数据规模较大且需要高计算效率的场景。
- **层次聚类**：适用于特征间存在相关性或希望理解数据的层次结构的情况，但不适合大规模数据。
- **参数模型聚类**：适用于概率分布相关的应用，特别适合数据质量评估和不确定性分析。

---

## 8. 课程安排提醒
- 本周四无课堂教学，但会发布K均值的练习活动。同学们可以通过该练习进一步掌握K均值聚类的实现和应用。