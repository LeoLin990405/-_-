### 第1周课程笔记：无监督学习与主成分分析（PCA）
---

### 一、课程概述
- **主题**：无监督学习（Unsupervised Learning），包括**主成分分析（Principal Component Analysis, PCA）**和**聚类分析（Clustering）**。
- **课程目标**：学习PCA的理论与实操应用，下周深入探讨聚类技术。
- **重要提醒**：
  - 学生应阅读选读材料，理解PCA的应用背景；
  - 课程会逐步增加编码练习，并依据Python或R语言偏好分组。

---

### 二、无监督学习概述
#### 1. 定义与目标
- **无监督学习的定义**：无监督学习是通过观察无标签数据中的特征关系，探索数据的分布模式和内在结构的方法。
- **无监督学习的目标**：
  - **数据模式发现**：找出数据中的特征关系和结构，如相似度和分布。
  - **数据可视化**：将高维数据降到低维空间，使数据结构更加直观；
  - **特征提取和降维**：去除不相关或冗余信息，减少特征数量，提高模型效率。

#### 2. 无监督学习与监督学习的区别
- **监督学习**：
  - 监督学习在已知输出变量$Y$的情况下，利用输入变量$X$进行训练以预测$Y$；
  - 常用于分类、回归等预测任务，模型通过优化预测误差提高性能；
  - 常见算法包括**线性回归**、**逻辑回归**、**决策树**等。

- **无监督学习**：
  - 无监督学习在没有输出标签的情况下进行分析，仅通过观察输入特征$X$的分布和结构来进行学习；
  - 常用于聚类分析、降维和数据可视化等任务，通过探索特征之间的关系来挖掘数据的潜在模式；
  - 常见算法包括**主成分分析（PCA）**和**聚类（K均值、层次聚类等）**。

#### 3. 半监督学习
- **定义**：半监督学习是结合监督和无监督学习的特性，通过少量标记数据帮助模型训练以推测未标记数据的特征。
- **特点**：尽管结合无标签数据，但仍依赖标记数据，主要应用于无法完全标记数据集的情况。

---

### 三、主成分分析（PCA）概述
#### 1. PCA的基本概念与目标
- **PCA定义**：主成分分析是一种降维技术，通过线性组合，将高维特征数据转换为低维特征，同时最大限度地保留数据的方差信息。
- **目标**：提取主要的数据信息、去除冗余特征，使数据结构更加清晰，以便在低维空间中进行进一步分析。

#### 2. PCA的应用场景
- **数据可视化**：将多维数据投影到2维或3维空间，帮助观察数据的分布特征。
- **特征提取与降维**：在特征间冗余信息较多时，通过减少特征数量消除噪声并提高模型效率；
- **去除噪声**：PCA可以帮助去除数据中的噪声，仅保留重要信息，提高数据质量。

---

### 四、PCA的数学原理与步骤
#### 1. PCA的数学定义与推导
PCA的核心思想是通过线性变换，将高维空间的数据特征转化为低维空间中的主成分。这些主成分在保留最多方差的同时可以帮助我们理解数据的主要模式。

##### 1.1 线性组合与主成分
- 对于包含 $p$ 个特征的数据集 $\mathbf{X} = [X_1, X_2, ..., X_p]$，我们希望找到一个新的变量集合 $\mathbf{Z} = [Z_1, Z_2, ..., Z_p]$，其中每个 $Z_i$ 都是原始特征的线性组合，形式如下：
  $$
  Z_i = w_{i1}X_1 + w_{i2}X_2 + \dots + w_{ip}X_p = \mathbf{w}_i^T \mathbf{X} 
  $$
  其中，$\mathbf{w}_i$ 表示第 $i$ 个主成分的权重向量或方向向量，$\mathbf{X}$ 为特征向量。

##### 1.2 方差最大化原则
- **目标**：第一个主成分 $Z_1$ 应尽可能多地解释数据的方差，使得主成分中包含的信息量最大化。
- **方差表达式**：假设 $\mathbf{X}$ 已中心化（即各特征均值为0），则第一个主成分 $Z_1$ 的方差可表示为：
  $$
  \text{Var}(Z_1) = \mathbf{w}_1^T \Sigma \mathbf{w}_1
  $$
  其中 $\Sigma$ 为 $\mathbf{X}$ 的协方差矩阵。
- **优化问题**：为了使得 $\text{Var}(Z_1)$ 最大化，我们希望找到一个向量 $\mathbf{w}_1$ 使得：
  $$
  \max_{\mathbf{w}_1} \ \mathbf{w}_1^T \Sigma \mathbf{w}_1
  $$
  约束条件：$\mathbf{w}_1^T \mathbf{w}_1 = 1$，确保权重向量 $\mathbf{w}_1$ 的长度为1。

##### 1.3 拉格朗日乘子法求解
- 为了求解这个最大化问题，可以引入拉格朗日乘子 $\lambda$，构造拉格朗日函数：
  $$
  L(\mathbf{w}_1, \lambda) = \mathbf{w}_1^T \Sigma \mathbf{w}_1 - \lambda (\mathbf{w}_1^T \mathbf{w}_1 - 1)
  $$
- 对 $\mathbf{w}_1$ 求导并设为 0，得到：
  $$
  \frac{\partial L}{\partial \mathbf{w}_1} = 2 \Sigma \mathbf{w}_1 - 2\lambda \mathbf{w}_1 = 0
  $$
  化简可得：
  $$
  \Sigma \mathbf{w}_1 = \lambda \mathbf{w}_1
  $$
  这说明 $\lambda$ 和 $\mathbf{w}_1$ 是协方差矩阵 $\Sigma$ 的特征值和特征向量。选择最大的特征值 $\lambda_1$ 及对应的特征向量 $\mathbf{w}_1$，即为第一个主成分的方向。

##### 1.4 后续主成分的推导
- 为得到第二个主成分 $Z_2$，要求其与 $Z_1$ 正交，即满足 $\mathbf{w}_2^T \mathbf{w}_1 = 0$，并且解释数据中剩余的最大方差。
- 通过递归该过程，依次求得主成分 $Z_3, Z_4, ..., Z_p$，对应特征值从大到小排列。

#### 2. PCA的实现步骤
1. **标准化数据**：将各特征标准化为均值为0，方差为1，以消除单位差异。
   - 对于特征$X_j$，标准化后得到$X_j'$：
     $$
     X_j' = \frac{X_j - \mu_j}{\sigma_j}
     $$
     其中$\mu_j$为特征$X_j$的均值，$\sigma_j$为特征$X_j$的标准差。

2. **计算协方差矩阵**：构建标准化数据的协方差矩阵$\Sigma$：
   $$
   \Sigma = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{X}_i - \mu)(\mathbf{X}_i - \mu)^T
   $$
   其中$\mathbf{X}_i$表示第$i$个样本，$\mu$为均值向量。

3. **特征值分解**：对协方差矩阵$\Sigma$进行特征值分解，得到特征值$\lambda_i$和对应的特征向量$\mathbf{w}_i$。
   - 特征值$\lambda_i$表示第$i$个主成分解释的方差大小；
   - 特征向量$\mathbf{w}_i$表示第$i$个主成分的方向。

4. **选择主成分**：通过计算方差贡献率（Proportion of Variance Explained, PVE）选择主要成分。第$i$个主成分的PVE计算公式为：
   $$ \text{PVE}_i = \frac{\lambda_i}{\sum_{j=1}^p \lambda_j}
      $$
      可以绘制碎石图（Scree Plot）观察PVE曲线，并根据“肘部”原则或方差累积达到一定比例（如85%）选择最优的主成分数量。

---

### 五、PCA的应用与案例分析
#### 案例1：财富指数的构建
- **目标**：利用PCA从受访者的财产数据中提取反映财富水平的潜在特征。
- **数据集**：包括7个变量：房屋数量、农田面积、自行车数量、鸡的数量、山羊数量、基本手机数量和智能手机数量。
  
##### 分析步骤与结果
1. **未缩放数据的PCA**：
   - 第一个主成分几乎完全由鸡的数量主导，解释了70%的方差；
   - 由于鸡的数量的方差最大，未缩放情况下PCA结果会偏向解释方差较大的特征。

2. **缩放后的PCA**：
   - 标准化后，第一个主成分的方差贡献降至24%，各变量的贡献更加均衡。
   - **主成分解读**：
     - **第一个主成分**：所有系数为正数，反映财产总体水平，可以解释为“财富指数”；
     - **第二个主成分**：包含正负系数，可能表示“城市化程度”，如智能手机数量和房屋数量较多的个体在此主成分上得分较高。

#### 案例2：文化价值观与政治知识
- **背景**：研究文化价值观（个人主义与平等主义）与政治知识水平的关系。
- **数据**：
  - 8个问题，用于评估个人主义和平等主义的倾向；
  - 5个政治知识问题，用于评估个体的政治知识水平，分为高、低两种知识层次。
  
##### 分析步骤与结果解读
1. **数据预处理**：
   - 将8个问题的选择数值化，并重新编码确保分数方向一致。
   - 对不同知识水平的个体分别进行PCA分析，比较主成分的方差解释率。

2. **结果解读**：
   - **高知识个体**：第一个主成分解释50%以上的方差，清晰地表现出从个人主义到平等主义的连续性；
   - **低知识个体**：方差解释分散，各主成分含义不明确，说明低知识个体的文化价值观不具一致性。

---

### 六、课程总结与安排
- **PCA的优势**：PCA通过提取主要成分帮助数据降维，去除噪声和冗余信息，广泛用于数据简化和特征提取。
- **PCA的局限**：在没有明确标签的情况下，主成分的解释具有主观性，解读易受人为偏见影响。

#### 后续安排
- 下节课进行PCA的代码实操，并根据语言偏好分组（Python或R）。
- 作业涉及PCA与聚类的应用，请提前熟悉代码和数据集。