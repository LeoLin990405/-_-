# **线性回归与模型评估 - 研究生级详细课程笔记**

## **1. 线性回归的概述与意义**
线性回归是统计学和机器学习中的基础模型，旨在描述 **自变量（特征）** 与 **因变量（目标变量）** 之间的线性关系。它不仅适用于简单的预测任务，还为理解复杂模型提供理论支撑。本笔记将深入介绍线性回归的 **数学推导、数据处理、模型评估**，以及如何在 **Python** 和 **R** 中实现。

---

## **2. 线性回归的数学推导与模型构建**

### **2.1 简单线性回归模型（Single Linear Regression）**

当模型只有一个输入变量 $x$ 和一个输出变量 $y$ 时，模型的表达式为：  
$y = \beta_0 + \beta_1 x + \epsilon$

其中：
- $y$：目标变量（因变量）。
- $x$：输入变量（自变量）。
- $\beta_0$：截距，表示 $x = 0$ 时的预测值。
- $\beta_1$：斜率，表示 $x$ 增加 1 单位时，$y$ 的变化量。
- $\epsilon$：误差项，假设 $\epsilon \sim N(0, \sigma^2)$。

### **2.2 多元线性回归模型（Multiple Linear Regression）**

当存在多个输入变量时，线性回归的形式为：  
$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i$  
或者用矩阵形式表示：  
$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}$$

其中：
- $\mathbf{y}$ 是 $n \times 1$ 的向量，表示 $n$ 个样本的目标变量。
- $\mathbf{X}$ 是 $n \times (p+1)$ 的矩阵，每一列是一个输入变量，每一行是一个样本。
- $\boldsymbol{\beta}$ 是 $(p+1) \times 1$ 的向量，表示待估计的回归系数。
- $\boldsymbol{\epsilon}$ 是 $n \times 1$ 的误差向量。

---

## **3. 最小二乘法（OLS）推导**

### **3.1 模型目标**
线性回归的目标是找到最优的系数 $\boldsymbol{\beta}$，使得预测值与真实值之间的 **残差平方和 (RSS)** 最小：
$$
\text{RSS} = \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2 = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2
$$

在矩阵形式下，残差平方和表示为：  
$$
\text{RSS} = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$

### **3.2 推导过程**
对 $\text{RSS}$ 关于 $\boldsymbol{\beta}$ 求偏导，并令其等于 0：  
$$
\frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) = 0
$$

解得：  
$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} = \mathbf{X}^\top \mathbf{y}
$$

因此，OLS 解为：  
$$
\boldsymbol{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

---

## **4. 模型评估指标**

### **4.1 均方误差 (MSE)**
$  
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2  
$

- **解释**：MSE 衡量预测值与真实值之间的误差平方的平均值，值越小，模型的预测效果越好。

### **4.2 根均方误差 (RMSE)**
$  
\text{RMSE} = \sqrt{\text{MSE}}  
$

- **解释**：RMSE 保持与原始数据一致的单位，便于理解预测误差的实际意义。

### **4.3 决定系数 $R^2$**
$  
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}  
$

- **解释**：$R^2$ 表示模型能解释的方差占总方差的比例。$R^2$ 的值越接近 1，模型的拟合效果越好。

### **4.4 调整后的 $R^2$**
$$
R^2_{\text{adjusted}} = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}  
$$
- **解释**：调整后的 $R^2$ 考虑了模型复杂度，避免在增加无关变量时 $R^2$ 虚假增大。

---

## **5. 数据划分与交叉验证**

### **5.1 训练集与测试集划分**
为了评估模型的泛化能力，通常将数据集划分为 **训练集** 和 **测试集**：

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

- **训练集**：用于训练模型，找到最优参数。
- **测试集**：用于评估模型在未见数据上的表现。

### **5.2 k 折交叉验证**
交叉验证是一种更稳定的模型评估方法，将数据集分为 $k$ 份，每次用 $k-1$ 份训练模型，剩余 1 份作为测试集，重复 $k$ 次：

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
print(f'Cross-Validation MSE: {-scores.mean()}')
```

---

## **6. 对数变换与数据预处理**

### **6.1 对数变换（Log Transformation）**
当输出变量分布偏斜时，对其进行对数变换可以使其更加符合正态分布，提高模型的线性拟合效果。

```python
import numpy as np

y_train_log = np.log(y_train)
y_test_log = np.log(y_test)

model_log = LinearRegression()
model_log.fit(X_train, y_train_log)

y_pred_log = model_log.predict(X_test)
mse_log = mean_squared_error(y_test_log, y_pred_log)
print(f'Log-MSE: {mse_log}')
```

---

## **7. Python 与 R 中的线性回归实现**

### **7.1 Python 实现**

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

data = pd.read_csv('mls22.csv')
X = data[['minutes', 'age', 'goals']]
y = data['salary']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f'MSE: {mean_squared_error(y_test, y_pred)}, R^2: {r2_score(y_test, y_pred)}')
```

### **7.2 R 实现**

```r
library(tidyverse)

data <- read_csv("mls22.csv")

set.seed(42)
train_idx <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

lm_model <- lm(salary ~ minutes + age + goals, data = train_data)
summary(lm_model)

predictions <- predict(lm_model, newdata = test_data)
mse <- mean((test_data$salary - predictions)^2)
cat("MSE:", mse, "\n")
```

---

## **8. 总结与思考**

### **8.1 核心总结**
- 线性回归模型通过最小二乘法找到最佳回归系数，并使用 $MSE$ 和 $R^2$ 等指标评估性能。
- 数据的正确划分与交叉验证有助于提高模型的稳定性与泛

化能力。
- 使用对数变换和缺失值处理等数据预处理技术，能有效提升模型表现。

### **8.2 作业与项目**
- **作业**：尝试使用不同变量组合构建线性回归模型，并比较其性能。
- **项目提案**：10 月 20 日提交，选择合适的数据集，阐述研究问题和分析思路。

通过本课程的深入学习，学生将掌握线性回归的理论推导与实战能力，为后续学习更加复杂的机器学习模型奠定坚实基础。