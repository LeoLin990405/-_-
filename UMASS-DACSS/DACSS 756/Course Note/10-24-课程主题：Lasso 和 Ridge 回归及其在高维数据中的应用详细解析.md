### 课程主题：Lasso 和 Ridge 回归及其在高维数据中的应用详细解析

---

### 课程公告
1. **作业3**：已发布，提交截止日期为11月8日。建议同学们尽早开始，以确保对正则化和特征选择方法的深入理解。
2. **退课提醒**：研究生退课的截止日期为10月29日。此日期之前退课不会影响GPA，但会在成绩单上标注为“VR”或“dropped”。

---

### 课程内容概述

本次课程深入讲解了几种常用于高维数据场景的正则化与特征选择方法，包括：
- 子集选择（Subset Selection）
- 主成分分析（PCA）
- Ridge回归
- Lasso回归
- Elastic Net

这些方法主要用于控制模型复杂度，防止过拟合，并提高模型的预测精度。以下为各方法的详细解析、数学推导及应用场景。

---

### 子集选择法（Subset Selection）

#### 1. 基本概念
子集选择法是一种模型选择方法，通过尝试不同的变量组合，寻找预测效果最优的模型。该方法适合用于特征数量较少且希望解释清晰的场景。子集选择法通常包括**全子集选择（Best Subset Selection）**和**逐步选择（Stepwise Selection）**：

- **全子集选择**：遍历所有可能的变量组合，计算每个组合的效果指标（如 $R^2$ 或 RSS），从中选取效果最佳的组合。
- **逐步选择**：逐步选择法包括前向选择和后向选择：
  - **前向选择**：从无变量的模型开始，每次加入一个对预测效果提升显著的变量。
  - **后向选择**：从全变量模型开始，每次移除一个对模型贡献较小的变量，直到找到最优组合。

#### 2. 数学表达式

假设我们有 $p$ 个预测变量 $X_1, X_2, \ldots, X_p$ 和因变量 $Y$，子集选择法的目标是找到最优变量子集 $S \subseteq \{1, 2, \ldots, p\}$，使得选择该子集的模型残差平方和（RSS）最小：

$$
\text{RSS}(S) = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j \in S} \beta_j X_{ij} \right)^2
$$

其中：
- $\beta_0$ 为截距项，$\beta_j$ 为每个特征的回归系数；
- $S$ 为所选择的变量子集。

**优化目标**是最小化 $\text{RSS}(S)$。

#### 3. 优点与缺点
- **优点**：可以通过选择关键变量来减少过拟合，并提升模型的解释性和预测性能。
- **缺点**：计算复杂度较高（特别是全子集选择），逐步选择方法可能会错过最佳变量组合，且子集选择法假设变量之间为线性关系，忽略了非线性或复杂的交互关系。

#### 4. 应用场景
适用于特征数量较少的情境，例如在医学研究中筛选影响疾病的关键因素。

---

### 主成分分析（Principal Component Analysis, PCA）

#### 1. 基本概念
主成分分析（PCA）是一种降维方法，旨在通过构建新的未相关主成分来减少数据维度，同时尽可能保留原始数据的信息。PCA基于数据的协方差矩阵，通过特征值分解得到线性组合的“主成分”，并以解释数据方差的几大主成分代替原始特征。

#### 2. 数学推导

假设数据矩阵为 $X \in \mathbb{R}^{n \times p}$，其中 $n$ 为样本数，$p$ 为特征数。目标是通过线性组合找到新坐标 $Z = XW$，使得新变量 $Z$ 能够解释数据的最大方差。具体步骤如下：

1. **数据中心化**：对 $X$ 中每个特征去均值化，使得每列均值为0，以消除不同量纲的影响。
   
2. **协方差矩阵计算**：计算标准化数据的协方差矩阵 $C = \frac{1}{n} X^T X$，反映变量之间的相关性。

3. **特征值分解**：对协方差矩阵 $C$ 进行特征值分解，得到特征值 $\lambda_1, \lambda_2, \ldots, \lambda_p$ 和对应的特征向量 $w_1, w_2, \ldots, w_p$。

4. **选择主成分**：按特征值从大到小排序，选择能够解释主要方差的前 $k$ 个特征向量作为主成分。

每个主成分 $Z_j$ 表示为：

$$
Z_j = X w_j
$$

其中 $w_j$ 是第 $j$ 个主成分的特征向量。

#### 3. 优缺点
- **优点**：能够在减少维度的同时保留数据的主要信息，有助于简化模型和数据处理。
- **缺点**：PCA假设变量间为线性关系，降维后的主成分难以直接解释，并可能忽略非线性关系。

#### 4. 应用场景
适用于高维数据的降维和简化场景，例如在金融风险分析中通过PCA对客户特征进行降维，以便更好地分析客户分层和预测风险。

---

### Ridge 回归（岭回归）

#### 1. 基本概念
Ridge回归是一种通过引入L2正则化项的回归方法，特别适用于应对多重共线性问题。Ridge回归在损失函数中加入了系数的平方和惩罚项，使得模型系数被限制在较小的范围内，从而降低模型复杂度并减少过拟合。

#### 2. 损失函数推导

Ridge回归的优化目标是最小化以下损失函数：

$$
L(\beta) = \text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2
$$

其中，$\text{RSS}$ 表示残差平方和（Residual Sum of Squares），计算方式为：

$$
\text{RSS} = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j X_{ij} \right)^2
$$

整体损失函数的表达式变为：

$$
L(\beta) = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j X_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

其中：
- $\lambda$ 为正则化参数，控制惩罚项的强度。较大的 $\lambda$ 会使回归系数 $\beta_j$ 更接近于0，从而简化模型。

#### 3. 最优解推导
对于标准化数据（每个特征去均值并单位化方差），Ridge回归的最优解为：

$$
\hat{\beta} = (X^T X + \lambda I)^{-1} X^T y
$$

其中，$I$ 是 $p \times p$ 的单位矩阵。

#### 4. 优缺点
- **优点**：通过缩小系数的大小来减少模型方差，从而降低过拟合风险，适合处理多重共线性问题。
- **缺点**：Ridge回归不会将系数缩减为0，因此无法进行特征选择，且引入了偏差，导致估计结果的偏移。

#### 5. 应用场景
Ridge回归适用于高维数据和多重共线性显著的数据集，例如在经济学和金融数据中用来提高模型的稳定性。

---

### Lasso 回归

#### 1. 基本概念
Lasso回归（Least Absolute Shrinkage and Selection Operator）引入了L1正则化项，使得某些回归系数能够缩减为0，从而实现特征选择。这使得Lasso回归非常适合在高维数据中选择关键特征。

#### 2. 损失函数推导

Lasso回归的优化目标是最小化以下损失函数：

$$
L(\beta) = \text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|
$$

其中 $\text{RSS}$ 表示残差平方和：

$$
\text{RSS} = \sum_{i=1

}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j X_{ij} \right)^2
$$

于是整体损失函数为：

$$
L(\beta) = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j X_{ij} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
$$

#### 3. 最优解推导（坐标下降法）

由于L1正则化项的不可微性，Lasso的优化解难以直接求得。常用的优化方法是**坐标下降法（Coordinate Descent）**，具体步骤如下：

1. 对每个系数 $\beta_j$，固定其他系数不变，只优化 $\beta_j$。
2. 更新 $\beta_j$ 的值，重复步骤1，直到所有系数收敛。

#### 4. 优缺点
- **优点**：Lasso可以自动将不重要的变量的系数缩减为0，实现特征选择。
- **缺点**：由于L1正则化的惩罚项，Lasso会引入偏差，从而导致估计结果偏离真实值，可能错过一些重要特征。

#### 5. 应用场景
Lasso回归适合在需要特征选择的高维数据场景中应用，例如基因数据分析中用于筛选与疾病相关的关键基因。

---

### Elastic Net 回归

#### 1. 基本概念
Elastic Net 是 Lasso 和 Ridge 的结合，通过同时引入 L1 和 L2 正则化项，平衡特征选择与系数收缩的需求。Elastic Net适合于存在较强特征相关性的高维数据。

#### 2. 损失函数推导

Elastic Net的损失函数为：

$$
L(\beta) = \text{RSS} + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
$$

其中：
- $\lambda_1$ 和 $\lambda_2$ 分别控制 L1 和 L2 正则化项的强度。

#### 3. 最优解推导（坐标下降法）

Elastic Net可以通过坐标下降法求解。具体步骤如下：
1. 对于每个系数 $\beta_j$，固定其他系数不变，优化 $\beta_j$。
2. 更新 $\beta_j$，重复此过程，直到所有系数收敛。

#### 4. 优缺点
- **优点**：Elastic Net同时具备Lasso和Ridge的优势，能够选择重要特征并适当收缩系数，适合高相关特征。
- **缺点**：需要同时调整两个正则化参数，模型调参复杂性较高。

#### 5. 应用场景
Elastic Net适用于高相关性特征的场景，例如在基因组数据分析中选择关键基因。

---

### 实验活动：Lasso 和 Ridge 回归的实际操作

通过模拟数据集来探讨Ridge、Lasso和Elastic Net回归的特性及其在高维数据中的应用效果。

1. **数据生成**：创建包含相关和不相关变量的模拟数据，观察不同正则化模型对特征选择的影响。
2. **模型拟合**：在R和Python中实现Ridge和Lasso回归，并比较其特征选择效果。
3. **结果分析**：对比OLS、Ridge和Lasso的回归系数，分析各方法在高维数据中的表现。
4. **Elastic Net拓展**：实现Elastic Net回归，探索其在处理相关性较强的特征集中的表现。

---

### 总结

本次课程详细分析了几种正则化和特征选择方法及其数学推导。选择合适的回归方法可以显著提升高维数据分析中的模型性能和稳定性。通过对Ridge、Lasso和Elastic Net的学习，研究者可以根据数据特点和分析需求选择合适的模型，从而更高效地进行预测和解释。