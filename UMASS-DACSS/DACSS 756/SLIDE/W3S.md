### DACSS 756: 分类（Week 3）详细笔记

#### 1. 分类概述
- **分类问题**涉及根据输入的特征向量 $X$ 来预测一个定性响应变量 $Y$，该变量取值于预定义的类别集合 $C$，如眼睛颜色（棕色、蓝色、绿色）或电子邮件类型（垃圾邮件或非垃圾邮件）。
  - 分类的目标是建立一个分类器 $C(X)$，它接受输入特征向量 $X$ 并预测 $Y$ 的类别。
  - 除了预测具体的类别，分类问题中通常还关注每个类别的概率。例如，在保险索赔的情况下，预测某个索赔是欺诈的概率比简单地判断它是否为欺诈更有价值，因为概率预测能为风险管理提供更精细的信息。

#### 2. 线性回归是否适用于分类？
- **线性回归**可以用于二元分类问题，即结果只有两个类别的情况（例如，信用卡是否违约：是或否）。在这种情形下，线性回归可以被当作一个分类器。
  - 这种做法在计量经济学中被称为“**线性概率模型**”（Linear Probability Model）。
  - 在线性回归中，我们试图建模 $ E[Y | X = x] $，它等同于 $ P(Y = 1 | X = x) $ 的估计。
  - 然而，线性回归会存在一定的问题，比如它可能会生成小于 0 或大于 1 的预测概率，而这在概率上是无效的。因此，相比于线性回归，**Logistic 回归**等专门设计来处理分类问题的方法更适合用于概率估计。

#### 3. 分类方法介绍
分类模型中有多种方法可以用于解决定性响应变量的预测问题，包括 **Logistic 回归**、**线性判别分析（LDA）**、**二次判别分析（QDA）** 和 **朴素贝叶斯**。这些方法的选择取决于数据的特点和建模的目的。

##### 3.1 Logistic 回归
- Logistic 回归是一种专门用于**二分类问题**的广义线性模型（GLM），其核心思想是使用**逻辑函数**将线性模型的输出映射到 $(0, 1)$ 区间，进而进行概率估计。
- **广义线性模型（GLM）**的组成部分：
  1. **概率分布**：描述生成过程的概率分布。对于 Logistic 回归，这个分布是**伯努利分布**（或二项式分布），因为目标变量是二元的。
  2. **线性模型**：$\eta = eta_0 + eta_1 X_1 + \dots + eta_p X_p$，即将输入特征与回归系数相乘并相加得到一个线性组合。
  3. **链接函数**：将线性组合的结果映射到目标变量的概率分布参数上。在 Logistic 回归中，使用的是**Logit 链接函数**，即 $ \text{logit}(p) = \log\left(\frac{p}{1-p}\right) $。
- **最大似然估计（MLE）**通常用于估计 Logistic 回归模型的参数。MLE 通过找到使得观测到的数据最有可能出现的参数值来进行估计。
  
##### 3.2 线性判别分析（LDA）和二次判别分析（QDA）
- **线性判别分析（LDA）**假设每个类别的特征是从多元正态分布中生成的，且不同类别具有相同的协方差矩阵。
  - LDA 的目标是找到能够最大化类别间距离并最小化类内方差的分割面。
  - LDA 适用于**类别分布具有相同方差的情况**，它构造一个线性决策边界来进行分类。
- **二次判别分析（QDA）**则放松了 LDA 的假设，允许每个类别具有不同的协方差矩阵。
  - QDA 适用于数据分布具有不同方差的情况，构造的决策边界为**二次曲线**。
  - 相较于 LDA，QDA 更灵活，但同时也需要更多的数据来估计每个类别的协方差矩阵。

##### 3.3 朴素贝叶斯分类器
- **朴素贝叶斯**是一种基于**贝叶斯定理**的分类算法，假设特征之间**相互独立**。
  - 贝叶斯定理用于计算某个实例属于某个类别的后验概率。
  - 尽管特征独立性假设在实际中可能不完全成立，但朴素贝叶斯在很多应用中表现良好，特别是当数据维度很高时。

#### 4. 模型性能评估
分类模型的性能评估通常基于模型对测试集数据的预测结果，通过一系列指标来衡量模型的表现。

##### 4.1 混淆矩阵（Confusion Matrix）
- **混淆矩阵**用于描述分类模型的预测结果，其中包含：
  - **真阳性（True Positive, TP）**：正确预测为正例的样本数。
  - **真阴性（True Negative, TN）**：正确预测为负例的样本数。
  - **假阳性（False Positive, FP）**：错误预测为正例的样本数（**I 型错误**）。
  - **假阴性（False Negative, FN）**：错误预测为负例的样本数（**II 型错误**）。

##### 4.2 性能指标
- **准确率（Accuracy）**：模型正确分类的样本数占总样本数的比例，计算公式为：
  $$
  \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
  $$
- **精确率（Precision）**：模型预测为正的样本中，实际为正的比例，计算公式为：
  $$
  \text{Precision} = \frac{TP}{TP + FP}
  $$
- **召回率（Recall）/敏感性（Sensitivity）**：实际为正的样本中被正确预测为正的比例，计算公式为：
  $$
  \text{Recall} = \frac{TP}{TP + FN}
  $$
- **特异性（Specificity）**：实际为负的样本中被正确预测为负的比例，计算公式为：
  $$
  \text{Specificity} = \frac{TN}{TN + FP}
  $$
- **F1 Score**：精确率和召回率的调和平均数，用于衡量模型的综合性能，计算公式为：
  $$
  F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  $$

##### 4.3 ROC 曲线与 AUC
- **ROC 曲线**（Receiver Operating Characteristic Curve）绘制了**真阳性率（TPR）**与**假阳性率（FPR）**在不同阈值下的变化。
- **AUC（Area Under the Curve）**表示 ROC 曲线下的面积，AUC 的取值范围为 $[0, 1]$，值越大表示模型的分类性能越好。

#### 5. Scikit-Learn 与 Tidymodels
##### 5.1 Scikit-Learn
- **Scikit-Learn** 是 Python 中非常流行的机器学习库，具有一致的 API 和丰富的文档，适用于各类机器学习任务。
- **Pipeline（流水线）**：
  - Pipeline 用于将数据预处理步骤（如**标准化**、**特征选择**等）与模型训练步骤串联在一起。
  - 通过流水线，可以保证数据的预处理只基于训练集，从而避免测试集信息泄露（数据泄露）。

##### 5.2 Tidymodels
- **Tidymodels** 是 R 语言中用于统一建模和机器学习任务的框架，旨在整合多种建模包。
- **Recipe**：
  - Recipe 用于定义数据预处理步骤，可以对数据进行**特征工程**、**归一化**等。
  - 使用类似 Tidyverse 的管道操作，Recipe 可以一步步对数据集进行处理。
- **Workflow**：
  - Workflow 类似于 Scikit-Learn 中的 Pipeline，包含 Recipe（预处理步骤）和 Model（建模步骤）。
  - Workflow 可以将数据预处理和建模组合在一起，简化建模流程。

#### 6. 缺失数据处理
- 数据集中常常存在缺失值，需要采取相应的处理策略。
- **缺失值类型**：
  1. **完全随机缺失（MCAR）**：缺失数据与其他变量无关，即缺失的概率在所有样本中是均匀的。这种情况下，直接删除缺失值或者用均值填补是可行的。
  2. **随机缺失（MAR）**：缺失值的出现与其他变量相关。例如，调查中高收入人群更可能不填写收入信息。在这种情况下，缺失值可以通过统计模型进行插补。
  3. **非随机缺失（MNAR）**：缺失的原因未知，且与变量本身的特定值有关，处理这种类型的缺失值较为困难，通常需要进一步探索数据。
  
- **训练集和测试集的分割**：
  - 在进行缺失值插补时，如果使用了涉及模型训练的方法，必须在训练集上进行，避免数据泄露。
  - 无论是 Tidymodels 还是 Scikit-Learn，都支持在预处理步骤中对缺失值进行插补，从而集成在完整的建模流水线中。

#### 7. 额外笔记
- **Scikit-Learn 的 Pipeline 与 Tidymodels 的 Workflow** 在机器学习建模中极为重要，它们使得模型训练和数据预处理更加模块化和可重用。
- 在分类问题中，合理选择模型和性能评估指标至关重要。不同模型适用于不同类型的数据，而选择合适的评估指标能够准确地反映模型的优缺点。

这些笔记为你深入理解 DACSS 756 第三周内容中的分类方法、模型选择、评估以及处理缺失值的技术提供了详细的总结。如果有任何问题或者需要进一步的说明，欢迎随时提问。