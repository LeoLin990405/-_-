### 1. 线性回归复习
#### 1.1 回归分析的概念和作用
- **回归分析（Regression Analysis）**是一种统计工具，用于研究一个或多个自变量（独立变量）与因变量（依赖变量）之间的关系，并通过这种关系预测因变量的值。回归分析通常用于以下场景：
  - **预测**：通过给定的输入数据预测未知的输出。例如，基于房屋的面积和位置来预测其售价。
  - **推断**：理解不同变量对结果的影响。例如，评估教育程度对收入水平的影响。

- **回归模型的基本形式**：
  - 线性回归的模型可以描述为：
    $$
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
    $$
    - $Y$ 是因变量，即我们想要预测或解释的变量。
    - $X_1, X_2, \ldots, X_p$ 是自变量，也叫做预测变量。
    - $\beta_0$ 是截距项，表示当所有自变量为 0 时，因变量的平均值。
    - $\beta_1, \beta_2, \ldots, \beta_p$ 是回归系数，表示每个自变量对因变量的影响。
    - $\epsilon$ 是误差项，代表模型未解释的部分。它包括随机误差和其他影响因素。

#### 1.2 线性回归的假设条件
- **线性回归模型建立在以下关键假设之上**：
  1. **线性关系（Linearity）**：假设因变量 $Y$ 与自变量 $X_1, X_2, \dots, X_p$ 之间的关系是线性的。这个假设意味着如果我们画出 $Y$ 与每个 $X$ 的关系图，它们应该近似成直线。
  2. **独立性（Independence）**：观测数据之间应该是独立的。特别是，误差项 $\epsilon$ 之间没有相关性。这意味着每一对样本的观测值都是独立的，这一假设非常重要，尤其是在时间序列数据中。
  3. **同方差性（Homoscedasticity）**：误差项的方差在所有自变量的取值范围内是恒定的。如果误差的方差随自变量的变化而变化，就会出现异方差问题，这会导致模型系数的估计不再最优。
  4. **正态性（Normality of Errors）**：误差项 $\epsilon$ 服从均值为 0 的正态分布。这一假设主要是为了推断和置信区间的准确性，但对于模型的预测能力，严格的正态性要求并不是必要的。

#### 1.3 线性回归模型的形式和解释
- **线性方程**：线性回归的方程描述了因变量 $Y$ 与一组自变量之间的线性关系：
  $$
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
  $$
  - **截距项 ($\beta_0$)**：表示当所有 $X$ 为 0 时，$Y$ 的平均值。它是模型的基准线。
  - **回归系数 ($\beta_j$)**：描述每个自变量对因变量的边际效应。例如，如果 $\beta_1 = 2$，这意味着 $X_1$ 每增加一个单位，$Y$ 增加 2 个单位（假设其他自变量保持不变）。
  - **随机误差项 ($\epsilon$)**：表示影响 $Y$ 的所有其他因素，且未被模型捕捉到的部分。误差项有助于捕捉现实中的不确定性。

### 2. 模型的用途和限制
#### 2.1 使用模型的目的
- **解释变量关系**：使用模型来分析自变量对因变量的影响。例如，在一个房价模型中，了解房屋面积、位置、年份等因素对价格的影响。
- **预测新数据**：一旦模型建立，可以将新的自变量值输入到模型中，以预测相应的因变量。例如，基于新的客户特征来预测销售可能性。

#### 2.2 模型的局限性
- **线性关系的假设**：线性回归假设 $Y$ 与 $X$ 的关系是线性的，但真实世界中的关系可能是非线性的。
- **对异常值敏感**：线性回归对数据中的异常值非常敏感，因为它试图最小化误差平方和，异常值可能会显著影响模型的估计结果。
- **多重共线性问题**：如果自变量之间高度相关，回归系数的估计可能不稳定，导致解释变得困难。这种现象称为多重共线性。

#### 2.3 "所有模型都是错的，但有些是有用的"
- **模型本质**：模型是对现实的简化，无法完全代表所有真实情况。然而，尽管它们并不完全准确，它们仍然可以为理解和预测复杂现象提供宝贵的洞察和帮助。

### 3. 线性回归的数学背景
#### 3.1 最小二乘法（Ordinary Least Squares, OLS）
- **目标**：最小二乘法的目标是找到一组回归系数，使得预测值与实际观测值之间的残差平方和（Residual Sum of Squares, RSS）最小。具体来说，RSS 定义为：
  $$
  RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left(y_i - \left(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}\right)\right)^2
  $$
  - **残差（Residuals）**：残差是指实际观测值与模型预测值之间的差异，即 $y_i - \hat{y}_i$。最小二乘法通过最小化这些残差的平方和来确定最佳的回归系数。

- **矩阵求解**：
  - 线性回归中的系数可以通过矩阵方法求解，简化为如下形式：
    $$
    \hat{\beta} = (X^T X)^{-1} X^T y
    $$
    - 这里：
      - $X$ 是设计矩阵，包含所有自变量的数据，每行代表一个观测样本，每列代表一个变量。
      - $y$ 是因变量的观测值向量。
      - $X^T$ 是矩阵 $X$ 的转置。
      - $(X^T X)^{-1}$ 表示 $X^T X$ 的逆矩阵，前提是 $X^T X$ 是可逆的。
  - 这个矩阵公式的核心是利用线性代数的方法来计算回归系数，简化了繁琐的计算过程。在计算机中，通常通过数值方法来解决这个矩阵求解问题。

#### 3.2 回归系数的解释
- **$\beta_0$**：截距，表示所有 $X_j$（自变量）为 0 时，因变量 $Y$ 的期望值。它可以被解释为模型的基准线或基准状态。
- **$\beta_j$ 的解释**：对于每个自变量 $X_j$，系数 $\beta_j$ 表示在其他所有变量保持不变的情况下，$X_j$ 增加一个单位时，因变量 $Y$ 的变化量。也称为边际效应（Marginal Effect）。

#### 3.3 回归系数的统计特性
- **估计量的期望**：在满足线性回归模型的假设条件下，最小二乘估计是无偏的，即：
  $$
  E(\hat{\beta}) = \beta
  $$
  - 这意味着，平均来看，估计的回归系数与真实值相符。
  
- **方差和标准误差**：为了评估回归系数的可靠性，我们还需要计算它们的方差和标准误差。标准误差越小，表明估计的系数越精确。

### 4. 预测与模型评估
#### 4.1 预测新观测值
- 一旦模型训练完成，就可以使用估计的系数来对新数据进行预测。对于新的观测值 $X = (x_{i1}, x_{i2}, \dots, x_{ip})$，预测值的计算公式如下：
  $$
  \hat{y} = \hat{\beta_0} + \hat{\beta_1} x_{i1} + \hat{\beta_2} x_{i2} + \cdots + \hat{\beta_p} x_{ip}
  $$
  - 这种预测可以应用于许多不同的场景，例如销售预测、需求预测等。

#### 4.2 线性回归模型的评估
- **残差分析**：通过分析残差（预测值与真实值之间的差异），可以判断模型是否适用。理想情况下，残差应该随机分布，没有系统性模式。
- **决定系数 $R^2$**：用于衡量模型的解释力，即自变量能够解释因变量的比例。它的取值范围为 0 到 1，$R^2$ 越高，表示模型对数据的拟合程度越好。
  $$
  R^2 = 1 - \frac{RSS}{TSS}
  $$
  - 其中 $RSS$ 是残差平方和，$TSS$ 是总平方和。$R^2$ 越接近于 1，说明模型解释了更多的因变量变化。
  
- **调整后的 $R^2$**：由于增加自变量会导致 $R^2$ 增大，调整后的 $R^2$ 考虑了模型中变量的数量，可以更好地评估模型的质量。

### 5. 在 R 和 Python 中实现线性回归
#### 5.1 R 中的线性回归
- **使用 `lm()` 函数**：
  - `lm()` 是 R 中用于执行线性回归的函数。这个函数自动将数据格式化为设计矩阵，使得线性回归的实现非常方便。
  - 示例代码：
  ```R
  model <- lm(Y ~ X1 + X2, data = my_data)
  summary(model)
  ```
  - `summary()` 函数会返回模型的详细信息，包括回归系数、标准误差、$p$ 值等，用于评估模型。

#### 5.2 Python 中的线性回归
- **`scikit-learn` 和 `statsmodels` 库**：
  - **`scikit-learn`**：
    - 主要用于机器学习任务，适合构建模型进行预测，但不提供统计学分析工具（如 $p$ 值）。
    - 示例代码：
    ```python
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    ```
  - **`statsmodels`**：
    - 更注重统计分析，提供类似于 R 的功能，用于评估模型的统计学性质。
    - 示例代码：
    ```python
    import statsmodels.api as sm
    X = sm.add_constant(X)  # 添加常数项（截距）
    model = sm.OLS(y, X).fit()
    print(model.summary())
    ```

### 6. MLS 球员薪资预测活动详细步骤
#### 6.1 任务背景和数据来源
- **任务描述**：本活动的目标是使用 2022 赛季 MLS 球员的表现数据来预测球员的薪资。数据集包含球员的年龄、进球数、助攻数等详细信息。
- **数据来源**：
  - 薪资数据来自 MLS 球员工会。
  - 球员表现数据来自 [fbref.com](https://fbref.com/)。

#### 6.2 操作步骤
1. **导入和加载数据**：
   - 使用 Python 的 `pandas` 库加载数据：
   ```python
   import pandas as pd
   mls22 = pd.read_csv("data/mls22.csv")
   ```
   - 查看数据集中的列名，以了解可以使用的变量：
   ```python
   print(mls22.columns)
   ```
   
2. **数据预处理**：
   - 将 `base_salary` 列的值转换为美元单位：
   ```python
   mls22['base_salary'] = mls22['base_salary'] * 100000
   ```
   - 检查是否存在缺失值，并进行相应处理。例如，可以用均值替换缺失值，或直接删除包含缺失值的样本：
   ```python
   mls22 = mls22.dropna()  # 删除缺失值
   ```
   
3. **探索性数据分析（EDA）**：
   - **数据描述**：通过 `.describe()` 函数查看数据的基本统计信息，包括均值、标准差、最小值和最大值：
   ```python
   print(mls22.describe())
   ```
   - **数据可视化**：绘制散点图来观察变量之间的关系，例如进球数与薪资之间的关系：
   ```python
   import matplotlib.pyplot as plt
   plt.scatter(mls22['Gls'], mls22['base_salary'])
   plt.xlabel('Goals')
   plt.ylabel('Base Salary')
   plt.show()
   ```
   
4. **数据集划分**：
   - 将数据分为训练集和测试集，通常是 70% 的数据用于训练，30% 的数据用于测试：
   ```python
   from sklearn.model_selection import train_test_split
   X = mls22[['Age', 'Gls', 'Ast']]  # 自变量
   y = mls22['base_salary']           # 因变量
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
   ```
   
5. **模型构建与拟合**：
   - 使用 `scikit-learn` 库中的 `LinearRegression` 来构建线性回归模型：
   ```python
   from sklearn.linear_model import LinearRegression
   model = LinearRegression()
   model.fit(X_train, y_train)
   ```
   - 打印模型的系数，解释每个特征对薪资的影响：
   ```python
   print("Intercept:", model.intercept_)
   print("Coefficients:", model.coef_)
   ```
   
6. **模型评估**：
   - 使用测试集来评估模型的预测能力，计算均方误差（Mean Squared Error, MSE）：
   ```python
   from sklearn.metrics import mean_squared_error
   y_pred = model.predict(X_test)
   mse = mean_squared_error(y_test, y_pred)
   print("Mean Squared Error:", mse)
   ```
   - 通过绘制实际值与预测值的对比图，来直观地了解模型的表现：
   ```python
   plt.scatter(y_test, y_pred)
   plt.xlabel('Actual Salary')
   plt.ylabel('Predicted Salary')
   plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
   plt.show()
   ```
   
7. **模型优化**：
   - 尝试添加更多的特征变量，例如比赛出场次数、进球贡献率等，以提高模型的预测精度。
   - **特征工程**：可以对某些特征进行转换，例如对薪资取对数来减少波动性，或者对变量进行标准化处理以避免不同量级的变量影响模型的表现。

### 7. 总结
- **线性回归是一种基础且强大的工具**，它能够帮助我们理解变量之间的关系，并对未来进行预测。虽然它的假设条件比较严格，但在很多场景下，线性回归依然是首选的模型，特别是当我们需要简单、易解释的结果时。
- **MLS 球员薪资预测活动**为我们提供了一个很好的实践机会，从数据导入、预处理、EDA、模型构建、评估到优化，每一个步骤都帮助我们理解如何将线性回归应用于现实问题中。

